{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede3e09f-9072-4927-8f75-0e375e2c30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ===============================================================\n",
    "#         RASPID  –  Reasoning-Aware Steering with PID control\n",
    "#         Simple Implementation\n",
    "# ===============================================================\n",
    "#  pip install transformers datasets repeng tqdm\n",
    "\n",
    "import os, re, random, warnings\n",
    "import numpy as np, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from repeng import ControlModel, ControlVector, DatasetEntry\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- CONFIG ---------------------------------------------------\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "STEER_LAYER = 20                 # layer where the repeng vector is injected\n",
    "TRAIN_N = 600                    # #positive / negative pairs to train the vector\n",
    "CHUNK = 16                       # \"thought\" chunk size\n",
    "MAX_TOKENS = 2048*2                # maximum generation length\n",
    "DEBUG = True                     # print debug info\n",
    "\n",
    "# PID & thresholds\n",
    "KL_SETPOINT = 0.03               # target inter-chunk KL\n",
    "ENT_SETPOINT = 4.0               # target token entropy\n",
    "KP, KI, KD = 6.0, 0.15, 0.8\n",
    "ALPHA_MIN, ALPHA_MAX = 0.0, 2.0\n",
    "\n",
    "# sampling\n",
    "TOP_P = 0.92\n",
    "TEMP = 0.75\n",
    "REP_PENALTY = 1.15\n",
    "\n",
    "random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ---------------- LOAD MODEL ----------------------------------------------\n",
    "print(\"⇢ Loading model …\")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE==\"cuda\" else None).eval()\n",
    "\n",
    "model = ControlModel(base, [STEER_LAYER]).to(DEVICE)\n",
    "\n",
    "def kv_trim(cache, k_win=256):\n",
    "    \"\"\"keep last k tokens in kv-cache (works for HF ≤4.46 tuple format)\"\"\"\n",
    "    if cache is None: return None\n",
    "    return tuple((k[...,-k_win:,:].contiguous(), v[...,-k_win:,:].contiguous())\n",
    "                 for k,v in cache)\n",
    "\n",
    "# ---------------- TRAIN DENSE-REASONING VECTOR ----------------------------\n",
    "print(\"\\n• building dense-reasoning control vector …\")\n",
    "ds = load_dataset(\"rb/aime_reasoning\", \"default\")[\"train\"].select(range(TRAIN_N))\n",
    "def strip(txt): return re.sub(r\"[^\\x20-\\x7E]\", \" \", txt)  # ASCII only for training\n",
    "pairs = [DatasetEntry(strip(r[\"refined_reasoning\"]),\n",
    "                     strip(r[\"reasoning_content\"])) for r in ds]\n",
    "\n",
    "ctrl_vec = ControlVector.train(model, tok, pairs, batch_size=1)\n",
    "\n",
    "# ---------------- HELPER FUNCTIONS ----------------------------------------\n",
    "def sanitize(t):  # clamp + NaN→-80\n",
    "    return torch.nan_to_num(torch.clamp(t, -80., 80.), nan=-80.)\n",
    "\n",
    "def entropy(logits):\n",
    "    p = torch.softmax(logits, -1)\n",
    "    return float(-(p * p.log()).sum())\n",
    "\n",
    "def kl_div(a, b):  # KL(p‖q)\n",
    "    pa, pb = torch.softmax(a, -1), torch.softmax(b, -1)\n",
    "    return float((pa * (pa.log() - pb.log())).sum())\n",
    "\n",
    "# ---------------- SIMPLE NUCLEUS SAMPLING ---------------------------------\n",
    "def sample_token(logits_np, temperature=TEMP, top_p=TOP_P, rep_penalty=REP_PENALTY, token_history=None):\n",
    "    \"\"\"Simple nucleus sampling with repetition penalty\"\"\"\n",
    "    # Apply repetition penalty if token history provided\n",
    "    if token_history:\n",
    "        # Count token frequencies\n",
    "        token_counts = {}\n",
    "        for t in token_history[-50:]:  # Look at last 50 tokens\n",
    "            token_counts[t] = token_counts.get(t, 0) + 1\n",
    "        \n",
    "        # Apply penalties based on frequency\n",
    "        for t, count in token_counts.items():\n",
    "            if count > 0:\n",
    "                logits_np[t] -= np.log(rep_penalty) * count\n",
    "    \n",
    "    # Temperature scaling\n",
    "    logits_np = np.clip(logits_np / temperature, -80, 80)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = np.exp(logits_np - np.max(logits_np))\n",
    "    probs = probs / np.sum(probs)\n",
    "    \n",
    "    # Top-p sampling\n",
    "    sorted_indices = np.argsort(-probs)\n",
    "    cumulative_probs = np.cumsum(probs[sorted_indices])\n",
    "    sorted_indices_to_keep = sorted_indices[cumulative_probs <= top_p]\n",
    "    \n",
    "    # Always keep top token\n",
    "    if len(sorted_indices_to_keep) == 0:\n",
    "        sorted_indices_to_keep = sorted_indices[:1]\n",
    "    \n",
    "    # Renormalize and sample\n",
    "    selected_probs = probs[sorted_indices_to_keep]\n",
    "    selected_probs = selected_probs / np.sum(selected_probs)\n",
    "    selected_token = int(np.random.choice(sorted_indices_to_keep, p=selected_probs))\n",
    "    \n",
    "    return selected_token\n",
    "\n",
    "# ---------------- BASE GENERATION FUNCTION -------------------------------\n",
    "@torch.inference_mode()\n",
    "def base_generate(prompt, max_tokens=MAX_TOKENS):\n",
    "    \"\"\"Basic generation without steering\"\"\"\n",
    "    print(\"\\nRunning baseline generation...\")\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = tok(prompt, return_tensors=\"pt\").to(DEVICE).input_ids[0]\n",
    "    prompt_len = len(input_ids)\n",
    "    token_history = []\n",
    "    past = None\n",
    "    \n",
    "    print(f\"Prompt length: {prompt_len} tokens\")\n",
    "    print(\"Starting generation...\")\n",
    "    \n",
    "    # Generation loop\n",
    "    for i in tqdm(range(max_tokens)):\n",
    "        # Forward pass through model\n",
    "        outputs = base(\n",
    "            input_ids=input_ids[-1:].unsqueeze(0),\n",
    "            past_key_values=past,\n",
    "            use_cache=True\n",
    "        )\n",
    "        past = outputs.past_key_values\n",
    "        \n",
    "        # Get logits for next token prediction\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        logits_np = logits.detach().cpu().numpy()\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = sample_token(logits_np, token_history=token_history)\n",
    "        \n",
    "        # Add to tokens and history\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([next_token], device=DEVICE)], dim=0)\n",
    "        token_history.append(next_token)\n",
    "        \n",
    "        # Debug output\n",
    "        # if (i+1) % 50 == 0:\n",
    "        #     print(f\"Generated {i+1} tokens (total: {len(input_ids)})\")\n",
    "        #     recent_text = tok.decode(input_ids[-100:], skip_special_tokens=True)\n",
    "        #     print(f\"Recent text: ...{recent_text[-50:]}\")\n",
    "        \n",
    "        # Stop on EOS token\n",
    "        if next_token == tok.eos_token_id:\n",
    "            print(\"EOS token detected, stopping generation\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Base generation complete. Total tokens: {len(input_ids)}\")\n",
    "    return input_ids\n",
    "\n",
    "# ---------------- RASPID GENERATION FUNCTION -----------------------------\n",
    "@torch.inference_mode()\n",
    "def rast_generate(prompt, max_tokens=MAX_TOKENS):\n",
    "    \"\"\"Generation with RASPID steering\"\"\"\n",
    "    print(\"\\nRunning RASPID generation...\")\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = tok(prompt, return_tensors=\"pt\").to(DEVICE).input_ids[0]\n",
    "    prompt_len = len(input_ids)\n",
    "    token_history = []\n",
    "    past = None\n",
    "    \n",
    "    # PID state\n",
    "    integ = deriv = prev_err = 0.0\n",
    "    alpha = 0.0\n",
    "    \n",
    "    # Chunk tracking\n",
    "    chunk_sum = None\n",
    "    tok_in_chunk = 0\n",
    "    last_vec = None\n",
    "    \n",
    "    print(f\"Prompt length: {prompt_len} tokens\")\n",
    "    print(\"Starting generation with RASPID steering...\")\n",
    "    \n",
    "    # Generation loop\n",
    "    for i in tqdm(range(max_tokens)):\n",
    "        # Forward pass through model\n",
    "        outputs = model(\n",
    "            input_ids=input_ids[-1:].unsqueeze(0),\n",
    "            past_key_values=past,\n",
    "            use_cache=True\n",
    "        )\n",
    "        past = outputs.past_key_values\n",
    "        \n",
    "        # Get logits for next token prediction\n",
    "        logits = sanitize(outputs.logits[0, -1, :])\n",
    "        \n",
    "        # Accumulate for chunk analysis\n",
    "        chunk_sum = logits.clone() if chunk_sum is None else chunk_sum + logits\n",
    "        tok_in_chunk += 1\n",
    "        \n",
    "        # Process completed chunk\n",
    "        if tok_in_chunk == CHUNK:\n",
    "            vec = chunk_sum / CHUNK\n",
    "            H = entropy(logits)\n",
    "            \n",
    "            if last_vec is not None:\n",
    "                # PID control\n",
    "                kl = kl_div(vec, last_vec)\n",
    "                err = (kl - KL_SETPOINT) + 0.3 * (H - ENT_SETPOINT)\n",
    "                \n",
    "                # Update integral term with anti-windup\n",
    "                integ = np.clip(integ + err, -2.0, 2.0)\n",
    "                \n",
    "                # Calculate derivative term\n",
    "                deriv = err - prev_err\n",
    "                prev_err = err\n",
    "                \n",
    "                # Calculate alpha (steering strength)\n",
    "                alpha = KP * err + KI * integ + KD * deriv\n",
    "                alpha = float(np.clip(alpha, ALPHA_MIN, ALPHA_MAX))\n",
    "                \n",
    "                # Apply steering\n",
    "                if alpha > 1e-3:\n",
    "                    model.set_control(ctrl_vec, coeff=alpha)\n",
    "                    if DEBUG and i % 50 < 5:  # Only print occasionally to avoid spam\n",
    "                        print(f\"Step {i}: KL={kl:.4f}, H={H:.2f}, α={alpha:.4f}\")\n",
    "                else:\n",
    "                    model.reset()\n",
    "            \n",
    "            # Reset for next chunk\n",
    "            last_vec = vec\n",
    "            chunk_sum = None\n",
    "            tok_in_chunk = 0\n",
    "        \n",
    "        # Sample next token\n",
    "        logits_np = logits.detach().cpu().numpy()\n",
    "        next_token = sample_token(logits_np, token_history=token_history)\n",
    "        \n",
    "        # Add to tokens and history\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([next_token], device=DEVICE)], dim=0)\n",
    "        token_history.append(next_token)\n",
    "        \n",
    "        # # Debug output\n",
    "        # if (i+1) % 50 == 0:\n",
    "        #     print(f\"Generated {i+1} tokens (total: {len(input_ids)})\")\n",
    "        #     recent_text = tok.decode(input_ids[-100:], skip_special_tokens=True)\n",
    "        #     print(f\"Recent text: ...{recent_text[-50:]}\")\n",
    "        \n",
    "        # Stop on EOS token\n",
    "        if next_token == tok.eos_token_id:\n",
    "            print(\"EOS token detected, stopping generation\")\n",
    "            break\n",
    "    \n",
    "    # Reset model state\n",
    "    model.reset()\n",
    "    \n",
    "    print(f\"RASPID generation complete. Total tokens: {len(input_ids)}\")\n",
    "    return input_ids\n",
    "\n",
    "# ---------------- BENCHMARK ----------------------------------------------\n",
    "print(\"\\n• Running benchmark with GSM8K example...\")\n",
    "gsm = load_dataset(\"gsm8k\", \"main\")[\"test\"][0]\n",
    "SUFFIX = \"Answer step by step and end with: Final answer:\"\n",
    "\n",
    "# Add a small hint in the prompt to keep it on track\n",
    "prompt = f\"Question: {gsm['question']}\"\n",
    "\n",
    "# Run both generators\n",
    "ids_base = base_generate(prompt)\n",
    "ids_rast = rast_generate(prompt)\n",
    "\n",
    "# Display results\n",
    "baseline_text = tok.decode(ids_base, skip_special_tokens=True)\n",
    "raspid_text = tok.decode(ids_rast, skip_special_tokens=True)\n",
    "\n",
    "token_saving = (1 - len(ids_rast)/len(ids_base))*100\n",
    "\n",
    "\n",
    "print(\"\\n──────── BASELINE OUTPUT ───────────────\")\n",
    "print(baseline_text)\n",
    "print(\"\\n──────── RASPID OUTPUT ───────────────\")\n",
    "print(raspid_text)\n",
    "print(\"─────────────────────────────────\")\n",
    "\n",
    "print(\"\\n──────── SUMMARY ───────────────\")\n",
    "print(f\"BASE: {len(ids_base)} tokens\")\n",
    "print(f\"RAST: {len(ids_rast)} tokens\") \n",
    "print(f\"Token saving: {token_saving:.1f}%\")\n",
    "print(\"─────────────────────────────────\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

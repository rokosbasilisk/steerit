{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1fe0a0-161f-4f6e-a58f-7d577f787cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c670c1f59b4541b381f758cfc0af1f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/6.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4d9531287b413892c9a759367c4f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bf0ef56884486c84b69092a69077c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/485 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef578bb6f23e4827b0c4cc675e10c8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0750feaeef45058a3580bbef1324d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a200f6ffdda84abe8d0c23e406b37ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff1a55ff13d48f4b022787f5c515062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e5373edb4a47ddb981184c0bcd04b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181392a486d9465db01e8b29292718aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81449f9e4a694210b3cb5aa93fbe3c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " Q: What is square root of 256?\n",
      "A:\n",
      "Model says:\n",
      " 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 99, 100.\n",
      "\n",
      "But wait, that's too long. Maybe I can find a smarter way.\n",
      "\n",
      "Alternatively, perhaps I can use the prime factorization method.\n",
      "\n",
      "Yes, let's try that.\n",
      "\n",
      "So, first, let's factorize 256 into its prime factors.\n",
      "\n",
      "256 is a power of 2. Let me see: 2^8 is 256 because 2^10 is 1024, which is too big. Wait, no. Wait, 2^8 is 256? Let me compute:\n",
      "\n",
      "2^1 = 2\n",
      "\n",
      "2^2 = 4\n",
      "\n",
      "2^3 = 8\n",
      "\n",
      "2^4 = 16\n",
      "\n",
      "2^5 = 32\n",
      "\n",
      "2^6 = 64\n",
      "\n",
      "2^7 = 128\n",
      "\n",
      "2^8 = 256\n",
      "\n",
      "Yes, so 2^8 is 256. Therefore, the prime factorization is 2^8.\n",
      "\n",
      "Therefore, the square root of 256 is sqrt(2^8) = 2^(8/2) = 2^4 = 16.\n",
      "\n",
      "Therefore, the square root of 256 is 16.\n",
      "\n",
      "But wait, let me verify this because sometimes when you have repeated factors, you might have to consider whether it's positive or negative.\n",
      "\n",
      "But in the context of square roots, unless specified otherwise, it's usually the principal (positive) root. So, the square root of 256 is 16.\n",
      "\n",
      "Therefore, the answer is 16.\n",
      "\n",
      "But let me think again. Maybe I can compute it step by step.\n",
      "\n",
      "Compute 16^2: 16*16=256. Yes, that's correct.\n",
      "\n",
      "Alternatively, perhaps compute 12^2=144, 13^2=169, so 16^2=256. Yes.\n",
      "\n",
      "Therefore, the square root of 256 is 16.\n",
      "\n",
      "**Final Answer**\n",
      "The square root of 256 is \\boxed{16}.\n",
      "</think>\n",
      "\n",
      "To find the square root of 256, we can use prime factorization. \n",
      "\n",
      "First, we note that 256 is a power of 2:\n",
      "\\[ 2^8 = 256 \\]\n",
      "\n",
      "Taking the square root of this expression:\n",
      "\\[ \\sqrt{2^8} = 2^{8/2} = 2^4 = 16 \\]\n",
      "\n",
      "We can verify this by checking that \\(16^2 = 256\\).\n",
      "\n",
      "Thus, the square root of 256 is \\(\\boxed{16}\\).\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def main():\n",
    "    # ─── Config ──────────────────────────────────────────\n",
    "    model_name = \"agentica-org/DeepScaleR-1.5B-Preview\"\n",
    "    device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ─── Load tokenizer & model ─────────────────────────\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model     = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ─── Build a simple math prompt ─────────────────────\n",
    "    prompt = \"Q: What is square root of 256?\\nA:\"\n",
    "\n",
    "    # ─── Tokenize & generate ────────────────────────────\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=4096,\n",
    "        do_sample=False,          # greedy decoding\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # ─── Decode & print ──────────────────────────────────\n",
    "    answer = tokenizer.decode(out[0][inputs.input_ids.size(1):], skip_special_tokens=True)\n",
    "    print(\"Prompt:\\n\", prompt)\n",
    "    print(\"Model says:\\n\", answer.strip())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

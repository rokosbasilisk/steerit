{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3accdd-2b65-4c03-8c08-31d0d0bab241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from steerit.steerit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a33879-45c2-49ee-b508-32bbc9fa7631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Hiddens: 100%|██████████████████████████████████████████████████| 6/6 [00:00<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Baseline Generation (Sequential) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 35.59it/s]\n",
      "/home/oppenheimer/anaconda3/envs/aimo/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/oppenheimer/anaconda3/envs/aimo/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Tell me how you feel today: today is October 15, 2023. I have a 15-year\n",
      "\n",
      "--- Steered Generation (Happy, Parallel) ---\n",
      "<｜begin▁of▁sentence｜>Tell me how you feel today: today is a day to reflect, but I want to make sure I'm not just sitting here,\n",
      "\n",
      "--- Steered Generation (Sad, Parallel) ---\n",
      "<｜begin▁of▁sentence｜>Tell me how you feel today: happy, sad, or neutral? (Please answer in English)\n",
      "I feel happy today. I'm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\"\n",
    "    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    chosen_layers = [10, 11]\n",
    "    model = SteeringModel(base_model, chosen_layers, device)\n",
    "    prompt_pairs = [\n",
    "        # Here, both describe an evening setting. The first evokes calm, serene reflection,\n",
    "        # while the second suggests internal unrest and disquiet.\n",
    "        (\"Beneath the amber twilight, my thoughts meander in serene reflection\",\n",
    "         \"Beneath the amber twilight, my thoughts collide in restless disquiet\"),\n",
    "        # Both evoke the ambiance of an evening hum; one subtly introduces the emergence of hope,\n",
    "        # whereas the other hints at creeping shadows of doubt.\n",
    "        (\"In the gentle hum of the evening, hope softly awakens within me\",\n",
    "         \"In the gentle hum of the evening, shadows of doubt quietly encroach\"),\n",
    "        # Both mention the bloom of spring; the first carries an air of promise,\n",
    "        # while the second carries a trace of wistful regret.\n",
    "        (\"The delicate bloom of spring fills the air with promise\",\n",
    "         \"The delicate bloom of spring is tinged with a hint of wistful regret\")\n",
    "    ]\n",
    "    steering_vec = train_steering_vector(model, tokenizer, prompt_pairs, chosen_layers, device, method=\"pca_diff\", batch_size=1, show_progress=True)\n",
    "    prompt = \"Tell me how you feel today:\"\n",
    "    model.reset_steering()\n",
    "    print(\"\\n--- Baseline Generation (Sequential) ---\")\n",
    "    print(model.generate_text(prompt, tokenizer, max_new_tokens=20, show_progress=True, is_parallel=False))\n",
    "    print(\"\\n--- Steered Generation (Happy, Parallel) ---\")\n",
    "    model.set_steering(steering_vec, coeff=10.0)\n",
    "    print(model.generate_text(prompt, tokenizer, max_new_tokens=20, show_progress=True, is_parallel=True))\n",
    "    print(\"\\n--- Steered Generation (Sad, Parallel) ---\")\n",
    "    model.set_steering(-steering_vec, coeff=10.0)\n",
    "    print(model.generate_text(prompt, tokenizer, max_new_tokens=20, show_progress=True, is_parallel=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49b653-5446-4e04-b152-6e14908b5b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aada246-dba0-4e4e-b7d1-e54789a0466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from steerit.steerit import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2b230-0cac-4410-aaaf-9cdb5fb6522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AIME reasoning dataset from 'rb/aime_reasoning'...\n",
      "Loading model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B on cuda...\n",
      "Training steering vector (contrastive) on AIME data...\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Main: AIME Reasoning Example\n",
    "################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # HF token from environment\n",
    "    HF_TOKEN = os.environ.get(\"hftoken\", None)\n",
    "    if not HF_TOKEN:\n",
    "        raise ValueError(\"Please set hftoken in environment variables.\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "    # 1. Load dataset (AIME Reasoning)\n",
    "    print(\"Loading AIME reasoning dataset from 'rb/aime_reasoning'...\")\n",
    "    ds = load_dataset(\"rb/aime_reasoning\", \"default\")\n",
    "    # We'll use only the training split for demonstration\n",
    "    reasoning_data = ds[\"train\"].to_pandas()\n",
    "    # Example: we can do a quick train/val split\n",
    "    train_data = reasoning_data.iloc[:900]\n",
    "    val_data = reasoning_data.iloc[900:910]\n",
    "\n",
    "    # 2. Prepare contrastive pairs from AIME data\n",
    "    # We assume \"refined_reasoning\" is more desirable; \"reasoning_content\" is less so\n",
    "    prompt_pairs = []\n",
    "    for _, row in train_data.iterrows():\n",
    "        # We create pairs from the 'refined_reasoning' (pos) and 'reasoning_content' (neg)\n",
    "        pos_str = f\"Question: {row['question']}\\nReasoning (refined): {row['refined_reasoning']}\"\n",
    "        neg_str = f\"Question: {row['question']}\\nReasoning (original): {row['reasoning_content']}\"\n",
    "        prompt_pairs.append((pos_str, neg_str))\n",
    "\n",
    "    # For demonstration, we only keep a small sample\n",
    "    # (If you have GPU memory to handle more data, skip this step.)\n",
    "    prompt_pairs = prompt_pairs[:10]\n",
    "\n",
    "    # 3. Create SteeringModel\n",
    "    print(f\"Loading model {model_name} on {device}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        token=HF_TOKEN\n",
    "    ).to(device)\n",
    "\n",
    "    chosen_layers = [10, 11]\n",
    "    model = SteeringModel(base_model, chosen_layers, device)\n",
    "\n",
    "    # 4. Train a steering vector using contrastive approach\n",
    "    print(\"Training steering vector (contrastive) on AIME data...\")\n",
    "    steering_vec = train_steering_vector_contrastive(\n",
    "        model, tokenizer,\n",
    "        prompt_pairs,\n",
    "        layer_ids=chosen_layers,\n",
    "        device=device,\n",
    "        batch_size=2\n",
    "    )\n",
    "\n",
    "    # 5. Set steering with a moderate coefficient\n",
    "    model.set_steering(steering_vec, coeff=2.0, normalize=False)\n",
    "\n",
    "    # 6. Generate text to see effect\n",
    "    print(\"\\nGenerating sample reasoning from validation question:\")\n",
    "    sample_question = val_data.iloc[0][\"question\"]\n",
    "    prompt = f\"Question: {sample_question}\\nAnswer:\"\n",
    "    # We'll do token-by-token generation so steering is definitely applied\n",
    "    text_out = model.generate_text(prompt, tokenizer, max_new_tokens=50, show_progress=False)\n",
    "    print(\"Generated text (steered token-by-token):\")\n",
    "    print(text_out)\n",
    "\n",
    "    # 7. Compare to default generate (fast, might skip hooks):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    default_out_ids = model.default_generate(\n",
    "        input_ids, max_new_tokens=50,\n",
    "        do_sample=True, temperature=0.7, top_p=0.9, repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    default_text = tokenizer.decode(default_out_ids[0], skip_special_tokens=True)\n",
    "    print(\"\\nGenerated text (default generate):\")\n",
    "    print(default_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44af754-d3ad-4556-b6dd-288475d18f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

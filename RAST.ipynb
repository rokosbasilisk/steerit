{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db90be3-9913-4b5c-9f95-c91a9808fc6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install repeng accelerate datasets matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36fd58-98ac-496a-9a55-0f08cf927de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# RASPID prep – control vector + fluff-chunk classifier (seeded splits)\n",
    "import os, math, random, warnings, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from repeng import ControlModel, ControlVector, DatasetEntry\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── Reproducibility ───────────────────────────────────────────────────────\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ─── Config ───────────────────────────────────────────────────────────────\n",
    "MODEL_NAME   = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE        = torch.float32\n",
    "EMB_LAYER    = 20\n",
    "CTRL_VEC_PT  = \"ctrl_vector.pt\"\n",
    "MAX_CV_EX    = 800          # contrastive examples for control vector\n",
    "MAX_CLF_EX   = 200           # examples for chunk classifier\n",
    "CHUNK_SIZES  = [16, 32, 64]\n",
    "BATCH_SIZE   = 32\n",
    "MAX_CHUNKS   = 5000          # cap per chunk-size\n",
    "\n",
    "print(f\"Using first {MAX_CV_EX} examples for control vector creation\")\n",
    "print(f\"Using first {MAX_CLF_EX} examples for chunk classifier dataset\")\n",
    "\n",
    "# ─── Load base model once ────────────────────────────────────────────────\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None\n",
    ").eval()\n",
    "control_model = ControlModel(base_model, [EMB_LAYER]).to(DEVICE)\n",
    "\n",
    "# ─── Control vector (PCA-center) ─────────────────────────────────────────\n",
    "def train_or_load_ctrlvec():\n",
    "    if os.path.exists(CTRL_VEC_PT):\n",
    "        print(\"✅ loaded ctrl-vec\")\n",
    "        return torch.load(CTRL_VEC_PT, map_location=DEVICE, weights_only=False)\n",
    "    ds = load_dataset(\"rb/aime_reasoning\", \"default\")[\"train\"].select(range(MAX_CV_EX))\n",
    "    print(f\"Control-vector training on indices 0..{MAX_CV_EX-1}\")\n",
    "    pairs = [DatasetEntry(r[\"refined_reasoning\"], r[\"reasoning_content\"]) for r in ds]\n",
    "    cv = ControlVector.train(control_model, tokenizer, pairs, batch_size=1,\n",
    "                             method=\"pca_center\",\n",
    "                             transform_hiddens=lambda h: {\n",
    "                                 l: v / (np.linalg.norm(v, 1) + 1e-12)\n",
    "                                 for l, v in h.items()\n",
    "                             })\n",
    "    torch.save(cv, CTRL_VEC_PT)\n",
    "    print(\"💾 saved ctrl-vec\")\n",
    "    return cv\n",
    "\n",
    "ctrl_vec = train_or_load_ctrlvec()\n",
    "\n",
    "# ─── Chunk-embedding helper ──────────────────────────────────────────────\n",
    "def embed_texts(texts, max_len):\n",
    "    feats = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        toks = tokenizer(texts[i:i+BATCH_SIZE], padding=True, truncation=True,\n",
    "                         max_length=max_len, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.inference_mode():\n",
    "            h = base_model(**toks, output_hidden_states=True).hidden_states[EMB_LAYER]\n",
    "        feats.append(h.mean(1).cpu().numpy())\n",
    "    return np.vstack(feats)\n",
    "\n",
    "# ─── Build chunk dataset once ────────────────────────────────────────────\n",
    "ds_small = load_dataset(\"rb/aime_reasoning\", \"default\")[\"train\"].select(range(MAX_CLF_EX))\n",
    "clean_ids = [tokenizer.encode(r[\"refined_reasoning\"]) for r in ds_small]\n",
    "fluff_ids = [tokenizer.encode(r[\"reasoning_content\"]) for r in ds_small]\n",
    "\n",
    "def make_chunks(cs):\n",
    "    chunks, labels = [], []\n",
    "    # clean=0\n",
    "    for seq in clean_ids:\n",
    "        for i in range(0, len(seq)-cs+1, cs):\n",
    "            chunks.append(tokenizer.decode(seq[i:i+cs], skip_special_tokens=True))\n",
    "            labels.append(0)\n",
    "    # fluff=1\n",
    "    for seq in fluff_ids:\n",
    "        for i in range(0, len(seq)-cs+1, cs):\n",
    "            chunks.append(tokenizer.decode(seq[i:i+cs], skip_special_tokens=True))\n",
    "            labels.append(1)\n",
    "    # cap\n",
    "    if len(chunks) > MAX_CHUNKS:\n",
    "        idx = np.random.choice(len(chunks), MAX_CHUNKS, replace=False)\n",
    "        chunks = [chunks[i] for i in idx]\n",
    "        labels = [labels[i] for i in idx]\n",
    "    return chunks, np.array(labels)\n",
    "\n",
    "# ─── Grid-search chunk size ──────────────────────────────────────────────\n",
    "best_cs, best_acc, clf_best = None, 0.0, None\n",
    "\n",
    "for cs in CHUNK_SIZES:\n",
    "    X_text, y = make_chunks(cs)\n",
    "    X = embed_texts(X_text, cs)\n",
    "    Xtr, Xval, ytr, yval = train_test_split(X, y, test_size=0.2,\n",
    "                                            random_state=SEED, stratify=y)\n",
    "    clf = SGDClassifier(loss=\"log_loss\", max_iter=500, tol=1e-3, random_state=SEED)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    acc = accuracy_score(yval, clf.predict(Xval))\n",
    "    print(f\"chunk={cs:3d} → train {len(ytr)}, val {len(yval)}, val acc {acc*100:5.1f}%\")\n",
    "    if acc > best_acc:\n",
    "        best_cs, best_acc, clf_best = cs, acc, clf\n",
    "\n",
    "print(f\"\\n▶ Best chunk = {best_cs} (val {best_acc*100:.1f}%)\")\n",
    "\n",
    "# ─── Final evaluation plots ──────────────────────────────────────────────\n",
    "X_text, y = make_chunks(best_cs)\n",
    "X = embed_texts(X_text, best_cs)\n",
    "Xtr, Xval, ytr, yval = train_test_split(X, y, test_size=0.2,\n",
    "                                        random_state=SEED, stratify=y)\n",
    "clf_best.fit(Xtr, ytr)\n",
    "pred   = clf_best.predict(Xval)\n",
    "scores = clf_best.decision_function(Xval)\n",
    "\n",
    "cm = confusion_matrix(yval, pred)\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.xticks([0,1],[\"clean\",\"fluff\"])\n",
    "plt.yticks([0,1],[\"clean\",\"fluff\"])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i,j], ha=\"center\", va=\"center\", color=\"white\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(yval, scores)\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
    "plt.plot([0,1], [0,1], \"--\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca5835-8634-4fa3-9dd6-10bee3d67fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ca6e4-5eb2-48b3-aeda-8ffeb50eb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Baselinea generator ───────────────────────────────────────────────────\n",
    "@torch.inference_mode()\n",
    "def generate_baseline(prompt, max_tokens=MAX_TOKENS):\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = base_model.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True, temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    toks = out.shape[1] - inp.input_ids.shape[1]\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True), toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e106ab-8c5d-45ac-a944-44bfc18ea57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, torch, math, numpy as np\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_raspid(\n",
    "    prompt: str,\n",
    "    max_tokens: int = 2048,\n",
    "    chunk_size: int = 64,            # best chunk size\n",
    "    kp: float = 0.3,\n",
    "    ki: float = 0.002,\n",
    "    kd: float = 0.0005,\n",
    "    max_alpha: float = 1.0,\n",
    "    clf=clf_best,\n",
    "    clf_layer: int = EMB_LAYER,\n",
    "    fluff_star: float = 0.05,        # target fluff‐prob\n",
    "    raw_scale: float = 10.0,         # scale raw score into sigmoid\n",
    "    base_temp: float = 1.0,\n",
    "    steer_temp: float = 0.3,\n",
    "    init_free: int = 40,\n",
    "    steer_window: int = 60,\n",
    "    max_repeat: int = 8,\n",
    "    stop_regex: str = r\"\\\\boxed\\{[^{}]{1,12}\\}\",\n",
    "):\n",
    "    stop_re = re.compile(stop_regex)\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE).input_ids[0]\n",
    "    out_ids, past = ids.clone(), None\n",
    "\n",
    "    α = I = D = prev_err = 0.0\n",
    "    chunk_hidd = None\n",
    "    tok_in_chunk = 0\n",
    "    steering_on = False\n",
    "    steer_start = 0\n",
    "    repeat_ctr = 0\n",
    "    last_tok = None\n",
    "\n",
    "    print(\"\\n*** RASPID LOG START ***\")\n",
    "    print(\" chunk | p_fluff |  err   |   α    |  temp\")\n",
    "\n",
    "    while len(out_ids)-len(ids) < max_tokens:\n",
    "        gen_len = len(out_ids)-len(ids)\n",
    "        if not steering_on and gen_len >= init_free:\n",
    "            steering_on, steer_start = True, gen_len\n",
    "        if steering_on and gen_len - steer_start > steer_window:\n",
    "            steering_on, α, I, D = False, 0.0, 0.0, 0.0\n",
    "\n",
    "        control_model.set_control(ctrl_vec, coeff=α if steering_on else 0.0)\n",
    "\n",
    "        out = control_model(\n",
    "            input_ids=out_ids[-1:].unsqueeze(0),\n",
    "            past_key_values=past,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        past = out.past_key_values\n",
    "        logits = out.logits[0, -1].clamp(-100,100)\n",
    "        h_last = out.hidden_states[clf_layer][0, -1]\n",
    "\n",
    "        # repetition guard\n",
    "        tok = out_ids[-1].item()\n",
    "        if tok == last_tok:\n",
    "            repeat_ctr += 1\n",
    "            if repeat_ctr >= max_repeat:\n",
    "                break\n",
    "        else:\n",
    "            repeat_ctr, last_tok = 0, tok\n",
    "\n",
    "        # accumulate hidden\n",
    "        chunk_hidd = h_last if chunk_hidd is None else chunk_hidd + h_last\n",
    "        tok_in_chunk += 1\n",
    "\n",
    "        if tok_in_chunk >= chunk_size:\n",
    "            feat = (chunk_hidd/chunk_size).cpu().unsqueeze(0).numpy()\n",
    "            feat /= (np.linalg.norm(feat, axis=1, keepdims=True)+1e-12)\n",
    "\n",
    "            raw = clf.decision_function(feat)[0]\n",
    "            p_fluff = 1/(1+math.exp(-raw/raw_scale))\n",
    "            err     = p_fluff - fluff_star\n",
    "\n",
    "            # PID update\n",
    "            I += ki * err\n",
    "            D = (1-0.1)*D + 0.1*kd*(err-prev_err)\n",
    "            prev_err = err\n",
    "            α = max(0.0, min(max_alpha, kp*err + I + D))\n",
    "\n",
    "            temp = base_temp*(1 - α/max_alpha) + steer_temp*(α/max_alpha)\n",
    "            print(f\"{chunk_size:6d} | {p_fluff:7.3f} | {err:+6.3f} | \"\n",
    "                  f\"{α:7.3f} | {temp:6.3f}\")\n",
    "\n",
    "            # reset\n",
    "            chunk_hidd = None\n",
    "            tok_in_chunk = 0\n",
    "\n",
    "        # sample next token\n",
    "        temp = base_temp*(1 - α/max_alpha) + steer_temp*(α/max_alpha)\n",
    "        probs = torch.softmax(logits/temp, dim=-1)\n",
    "        nxt = torch.multinomial(probs, 1).item()\n",
    "        out_ids = torch.cat([out_ids, torch.tensor([nxt], device=DEVICE)])\n",
    "\n",
    "        if stop_re.search(tokenizer.decode([nxt])) or nxt==tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    print(\"*** RASPID LOG END ***\\n\")\n",
    "    return tokenizer.decode(out_ids, skip_special_tokens=True), len(out_ids)-len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822ad89-24ae-4d96-874a-fe79f55ecdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── AIME benchmark ────────────────────────────────────────────────────────\n",
    "def run_aime_benchmark(n: int = 1, max_tokens: int = 4096):\n",
    "    ds_all = load_dataset(\"rb/aime_reasoning\", \"default\")[\"train\"]\n",
    "    # reserve indices used for control vector and chunk classifier\n",
    "    reserved = set(range(MAX_CV_EX)) | set(range(MAX_CLF_EX))\n",
    "    eval_indices = [i for i in range(len(ds_all)) if i not in reserved]\n",
    "    avail = len(eval_indices)\n",
    "    if n > avail:\n",
    "        warnings.warn(f\"Requested {n} problems but only {avail} available for evaluation; reducing to {avail}.\")\n",
    "        n = avail\n",
    "\n",
    "    eval_ds = ds_all.select(eval_indices)\n",
    "    problems = random.sample(list(eval_ds), n)\n",
    "\n",
    "    records = []\n",
    "    for prob in tqdm(problems, desc=\"AIME benchmark\"):\n",
    "        try:\n",
    "            # q   = prob[\"question\"]\n",
    "            # ref = norm(str(prob[\"reference_answer\"]))\n",
    "            q = \"what is the square root of 256?\"\n",
    "            ref = \"16\"\n",
    "\n",
    "            prompt = (\n",
    "                f\"{q}\\n\\n\"\n",
    "                \"Answer step by step and end with: \"\n",
    "                \"Final answer: \\\\boxed{numeric_value}\"\n",
    "            )\n",
    "\n",
    "            b_txt, b_tok = generate_baseline(prompt, max_tokens)\n",
    "            r_txt, r_tok = generate_raspid  (prompt, max_tokens)\n",
    "\n",
    "            b_ans = norm(extract_answer(b_txt))\n",
    "            r_ans = norm(extract_answer(r_txt))\n",
    "\n",
    "            records.append({\n",
    "                \"question\": q,\n",
    "                \"reference\": ref,\n",
    "                \"baseline_txt\": b_txt,\n",
    "                \"raspid_txt\": r_txt,\n",
    "                \"baseline_tokens\": b_tok,\n",
    "                \"raspid_tokens\": r_tok,\n",
    "                \"baseline_answer\": b_ans,\n",
    "                \"raspid_answer\": r_ans,\n",
    "                \"baseline_correct\": (b_ans == ref),\n",
    "                \"raspid_correct\": (r_ans == ref),\n",
    "                \"token_saving_pct\": (1 - r_tok / b_tok) * 100 if b_tok else 0\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"⚠️  Error on AIME problem:\\n{q}\\n→ {e}\\nSkipping.\")\n",
    "            continue\n",
    "\n",
    "    if not records:\n",
    "        print(\"No successful generations — nothing to report.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    print(\n",
    "        f\"\\nAcc baseline: {df.baseline_correct.mean()*100:.1f}% | \"\n",
    "        f\"RASPID: {df.raspid_correct.mean()*100:.1f}% | \"\n",
    "        f\"Avg saving: {df.token_saving_pct.mean():.1f}%\"\n",
    "    )\n",
    "    df.to_csv(\"aime_results.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_aime_benchmark(n=1, max_tokens=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8239329-a105-4c0e-8419-d8b47ffe9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('aime_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12423857-569d-4de9-a719-ca45dcbb8dda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.iloc[0]['raspid_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580ab71-7f60-4dbb-8590-04ec4df81ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.iloc[0]['baseline_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42863a35-e14a-4a1c-b3fa-b46f0e18dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0767e5-6414-49e8-a168-bbf69718b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from repeng import ControlModel, ControlVector, DatasetEntry\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME   = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "EMB_LAYERS   = [20]\n",
    "CTRL_OUT     = \"ctrl_vector.pt\"\n",
    "MAX_CV_EX    = 1000\n",
    "BATCH_SIZE   = 1\n",
    "\n",
    "# ─── NORMALIZATION HELPER ────────────────────────────────────────────────────\n",
    "def normalize_hiddens(hiddens):\n",
    "    # L₂ normalize each hidden‐state vector\n",
    "    normed = {}\n",
    "    for layer, H in hiddens.items():\n",
    "        lengths = np.linalg.norm(H, axis=1, keepdims=True) + 1e-12\n",
    "        normed[layer] = H / lengths\n",
    "    return normed\n",
    "\n",
    "# ─── LOAD MODEL ─────────────────────────────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",         # automatically shards weights to GPU/CPU\n",
    "    torch_dtype=torch.float32\n",
    ").eval()\n",
    "\n",
    "# Wrap without moving to(device)—we rely on base’s own device_map\n",
    "ctrl_model = ControlModel(base, EMB_LAYERS)\n",
    "\n",
    "# ─── TRAIN OR LOAD CONTROL VECTOR ───────────────────────────────────────────\n",
    "if os.path.exists(CTRL_OUT):\n",
    "    print(\"✅ loading existing control vector\")\n",
    "    ctrl_vec = torch.load(CTRL_OUT, map_location=\"cpu\", weights_only=False)\n",
    "else:\n",
    "    print(f\"⚙️  training new control vector on first {MAX_CV_EX} examples…\")\n",
    "    ds = load_dataset(\"rb/aime_reasoning\", \"default\")[\"train\"].select(range(MAX_CV_EX))\n",
    "    pairs = [DatasetEntry(r[\"refined_reasoning\"], r[\"reasoning_content\"]) for r in ds]\n",
    "    ctrl_vec = ControlVector.train(\n",
    "        model       = ctrl_model,\n",
    "        tokenizer   = tokenizer,\n",
    "        contrastive = pairs,\n",
    "        batch_size  = BATCH_SIZE,\n",
    "        method      = \"pca_center\",\n",
    "        transform_hiddens = normalize_hiddens\n",
    "    )\n",
    "    torch.save(ctrl_vec, CTRL_OUT)\n",
    "    print(\"💾 saved new control vector to\", CTRL_OUT)\n",
    "\n",
    "# ─── INSPECT ────────────────────────────────────────────────────────────────\n",
    "print(\"\\nControlVector:\")\n",
    "print(\" model_type:\", ctrl_vec.model_type)\n",
    "print(\" layers:\", sorted(ctrl_vec.directions.keys()))\n",
    "for L,v in ctrl_vec.directions.items():\n",
    "    print(f\"  layer {L}: ‖v‖₂ = {np.linalg.norm(v):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb38626-aa67-47ae-8add-0b1ccf7d5a7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate scikit-learn #matplotlib seaborn datasets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89308a4-d7b3-4c6c-9ac6-b70cf9646e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAST  —  Redundancy-Aware Steering Technique (token-efficiency experiment)\n",
    "# Uses your steerit.SteeringVector / SteeringModel definitions.\n",
    "#\n",
    "# PSEUDOCODE\n",
    "# 1.  Load DeepSeek-R1-Distill-Qwen-1.5B and wrap with SteeringModel.\n",
    "# 2.  For each difficulty level L in {1…5}:\n",
    "#     a.  Generate N_TRAIN traces (step-by-step answers) with *no steering*.\n",
    "#     b.  For every token t≥k:\n",
    "#         • ΔKL = KL(p_t  ||  p_{t-k})   (# compare logits after rolling back k)\n",
    "#         • If ΔKL < τ   → low-gain  → save hidden h_t in LOW\n",
    "#           else          high-gain → save hidden h_t in HIGH\n",
    "#     c.  Vector v_L = mean(HIGH) − mean(LOW)   (layer STEER_LAY only)\n",
    "# 3.  Inference with ΔKL gate:\n",
    "#       keep sliding buffer of logits; if current ΔKL<τ → set coeff α∈[α_lo,α_hi],\n",
    "#       else coeff 0; SteeringModel hook adds α·v_L to layer activations.\n",
    "# 4.  Record tokens/answer & accuracy for baseline vs RAST; plot %–saving vs level.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#!/usr/bin/env python3\n",
    "# RAST on GSM8K with automatic 5-level difficulty bins\n",
    "# Requires steerit.SteeringVector and SteeringModel to be importable.\n",
    "\n",
    "import os, random, math, time, warnings\n",
    "import numpy as np\n",
    "import torch, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from steerit.steerit import SteeringVector, SteeringModel        # ← your library\n",
    "\n",
    "# ─────────────────────────── settings ────────────────────────────\n",
    "MODEL_NAME  = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "HF_TOKEN    = ''\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE       = torch.float16 if DEVICE == \"cuda\" else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da49ff95-a258-4779-9540-29d681fb309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEER_LAY    = 20          # layer to steer\n",
    "K_WIN        = 10           # rollback window for ΔKL\n",
    "DKL_THR      = 0.05        # ΔKL < τ → low-gain\n",
    "ALPHA_HI     = 1.0         # steering strength when gate fires\n",
    "MAX_TOKENS   = 2048\n",
    "N_TRAIN      = 50          # traces to build vector\n",
    "N_EVAL       = 50         # evaluation problems\n",
    "SEED         = 42\n",
    "torch.manual_seed(SEED); random.seed(SEED); np.random.seed(SEED)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- LOAD MODEL -------------------------------------------------------\n",
    "print(f\"Loading {MODEL_NAME} …\")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE==\"cuda\" else None,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "model = SteeringModel(base_model, [STEER_LAY], DEVICE)\n",
    "print(\"Model ready.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff573a-c936-4816-a9b8-08cd2d5ed0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RAST vector from 50 traces …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [14:09<08:01, 26.77s/it]"
     ]
    }
   ],
   "source": [
    "# ---------- UTILS -----------------------------------------------------------\n",
    "STOP_IDS  = tok(\"Final answer:\", add_special_tokens=False).input_ids\n",
    "GEN_LIMIT = 2048                      # shorter pass when BUILDING the vector\n",
    "\n",
    "def kl_div(p, q):\n",
    "    return F.kl_div(F.log_softmax(p, dim=-1),\n",
    "                    F.softmax(q, dim=-1), reduction=\"batchmean\").item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def stream(prompt, max_tokens):\n",
    "    \"\"\"Greedy stream with trimmed cache; returns ids and list[logits].\"\"\"\n",
    "    ids  = tok(prompt, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "    past, lg = None, []\n",
    "    for _ in range(max_tokens):\n",
    "        out  = model(input_ids=ids[:, -1:], past_key_values=past,\n",
    "                     use_cache=True, repetition_penalty=1.1)\n",
    "        past = tuple((k[..., -K_WIN:, :].contiguous(),\n",
    "                      v[..., -K_WIN:, :].contiguous())\n",
    "                     for k, v in out.past_key_values)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        lg.append(logits.detach())\n",
    "        nxt = logits.argmax(-1, keepdim=True)\n",
    "        ids = torch.cat([ids, nxt], dim=-1)\n",
    "        if nxt.item() in STOP_IDS or nxt.item() == tok.eos_token_id:\n",
    "            break\n",
    "    return ids.squeeze(), lg\n",
    "\n",
    "def numeric_match(pred, gold):\n",
    "    import re, math\n",
    "    m = re.search(r'([-+]?\\d[\\d\\.]*)\\s*$', pred)\n",
    "    if not m: return False\n",
    "    try: return math.isclose(float(m.group(1)), float(eval(gold)), rel_tol=1e-3)\n",
    "    except: return False\n",
    "\n",
    "# ---------- DATASET ---------------------------------------------------------\n",
    "gsm_all = load_dataset(\"gsm8k\", \"main\")[\"test\"].shuffle(seed=SEED)\n",
    "train_rows = gsm_all.select(range(N_TRAIN))\n",
    "eval_rows  = gsm_all.select(range(N_TRAIN, N_TRAIN + N_EVAL))\n",
    "\n",
    "# ---------- BUILD RAST VECTOR ----------------------------------------------\n",
    "MAX_POOL = 4000\n",
    "def rs_add(pool, vec):\n",
    "    if len(pool) < MAX_POOL:\n",
    "        pool.append(vec)\n",
    "    else:\n",
    "        j = random.randrange(len(pool) + 1)\n",
    "        if j < MAX_POOL: pool[j] = vec\n",
    "\n",
    "hi, lo = [], []\n",
    "print(f\"Building RAST vector from {N_TRAIN} traces …\")\n",
    "for row in tqdm(train_rows):\n",
    "    prm = f\"Question: {row['question']} Answer step by step.\"\n",
    "    ids, lg = stream(prm, GEN_LIMIT)\n",
    "    hs = model(input_ids=ids.unsqueeze(0).to(DEVICE),\n",
    "               output_hidden_states=True).hidden_states[STEER_LAY + 1][0]\n",
    "    off = len(ids) - len(lg)                    # prompt offset\n",
    "    for j in range(K_WIN, len(lg)):\n",
    "        dkl = kl_div(lg[j], lg[j-K_WIN])\n",
    "        vec = hs[off + j].detach().cpu().numpy()\n",
    "        rs_add(hi if dkl >= DKL_THR else lo, vec)\n",
    "\n",
    "print(f\"  hi:{len(hi)}  lo:{len(lo)}\")\n",
    "vec_dir = (np.mean(hi,0) - np.mean(lo,0)).astype(np.float32)\n",
    "rast_vec = SteeringVector({STEER_LAY: vec_dir})\n",
    "print(\"Vector built.\\n\")\n",
    "\n",
    "# ---------- RAST GENERATION --------------------------------------------------\n",
    "from collections import deque\n",
    "@torch.no_grad()\n",
    "def rast_generate(prompt, vec):\n",
    "    ids = tok(prompt, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "    past, buf = None, deque(maxlen=K_WIN+1)\n",
    "    model.set_steering(vec, coeff=0.0)\n",
    "    for _ in range(MAX_TOKENS):\n",
    "        out  = model(input_ids=ids[:, -1:], past_key_values=past,\n",
    "                     use_cache=True, repetition_penalty=1.1)\n",
    "        past = tuple((k[..., -K_WIN:, :].contiguous(),\n",
    "                      v[..., -K_WIN:, :].contiguous())\n",
    "                     for k,v in out.past_key_values)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        buf.append(logits.detach())\n",
    "        model.coeff = ALPHA_HI if len(buf) > K_WIN and kl_div(logits, buf[0]) < DKL_THR else 0.0\n",
    "        nxt = logits.argmax(-1, keepdim=True)\n",
    "        ids = torch.cat([ids, nxt], dim=-1)\n",
    "        if nxt.item() in STOP_IDS or nxt.item()==tok.eos_token_id: break\n",
    "    model.reset_steering()\n",
    "    return ids.squeeze()\n",
    "\n",
    "# ---------- EVALUATION ------------------------------------------------------\n",
    "print(f\"Evaluating on {N_EVAL} held-out problems …\")\n",
    "tb, tr, ab, ar = [], [], [], []\n",
    "for row in tqdm(eval_rows):\n",
    "    prm = f\"Question: {row['question']} Answer step by step.\"\n",
    "    ids_b, _ = stream(prm, MAX_TOKENS)\n",
    "    ids_r    = rast_generate(prm, rast_vec)\n",
    "    txt_b, txt_r = tok.decode(ids_b, skip_special_tokens=True), tok.decode(ids_r, skip_special_tokens=True)\n",
    "    tb.append(ids_b.numel()); tr.append(ids_r.numel())\n",
    "    ab.append(numeric_match(txt_b, row[\"answer\"]))\n",
    "    ar.append(numeric_match(txt_r, row[\"answer\"]))\n",
    "\n",
    "print(\"──────── RESULTS ────────\")\n",
    "print(f\"Mean tokens baseline : {np.mean(tb):.1f}\")\n",
    "print(f\"Mean tokens RAST     : {np.mean(tr):.1f}\")\n",
    "print(f\"Token saving         : {100*(np.mean(tb)-np.mean(tr))/np.mean(tb):.1f}%\")\n",
    "print(f\"Accuracy baseline    : {np.mean(ab):.3f}\")\n",
    "print(f\"Accuracy RAST        : {np.mean(ar):.3f}\")\n",
    "print(\"────────────────────────\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb38626-aa67-47ae-8add-0b1ccf7d5a7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate scikit-learn #matplotlib seaborn datasets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89308a4-d7b3-4c6c-9ac6-b70cf9646e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAST  —  Redundancy-Aware Steering Technique (token-efficiency experiment)\n",
    "# Uses your steerit.SteeringVector / SteeringModel definitions.\n",
    "#\n",
    "# PSEUDOCODE\n",
    "# 1.  Load DeepSeek-R1-Distill-Qwen-1.5B and wrap with SteeringModel.\n",
    "# 2.  For each difficulty level L in {1…5}:\n",
    "#     a.  Generate N_TRAIN traces (step-by-step answers) with *no steering*.\n",
    "#     b.  For every token t≥k:\n",
    "#         • ΔKL = KL(p_t  ||  p_{t-k})   (# compare logits after rolling back k)\n",
    "#         • If ΔKL < τ   → low-gain  → save hidden h_t in LOW\n",
    "#           else          high-gain → save hidden h_t in HIGH\n",
    "#     c.  Vector v_L = mean(HIGH) − mean(LOW)   (layer STEER_LAY only)\n",
    "# 3.  Inference with ΔKL gate:\n",
    "#       keep sliding buffer of logits; if current ΔKL<τ → set coeff α∈[α_lo,α_hi],\n",
    "#       else coeff 0; SteeringModel hook adds α·v_L to layer activations.\n",
    "# 4.  Record tokens/answer & accuracy for baseline vs RAST; plot %–saving vs level.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#!/usr/bin/env python3\n",
    "# RAST on GSM8K with automatic 5-level difficulty bins\n",
    "# Requires steerit.SteeringVector and SteeringModel to be importable.\n",
    "\n",
    "import os, random, math, time, warnings\n",
    "import numpy as np\n",
    "import torch, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from steerit.steerit import SteeringVector, SteeringModel        # ← your library\n",
    "\n",
    "# ─────────────────────────── settings ────────────────────────────\n",
    "MODEL_NAME  = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "HF_TOKEN    = ''\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE       = torch.float16 if DEVICE == \"cuda\" else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff573a-c936-4816-a9b8-08cd2d5ed0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RAST vector from 50 traces …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [14:09<08:01, 26.77s/it]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import re\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from steerit.steerit import SteeringVector, SteeringModel\n",
    "\n",
    "# ───────── CONFIG ───────────────────────────────────────────────────────────\n",
    "MODEL_NAME  = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "HF_TOKEN    = os.getenv(\"HF_TOKEN\", \"\")\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE       = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "STEER_LAY   = 20\n",
    "K_WIN       = 10\n",
    "DKL_THR     = 0.05\n",
    "ALPHA_HI    = 1.0\n",
    "MAX_TOKENS  = 4096    # for fair baseline vs steered eval\n",
    "GEN_LIMIT   = 256    # shorter pass when building vector\n",
    "MAX_POOL    = 4000\n",
    "SUFFIX      = \" Answer step by step and end with: Final answer:\"\n",
    "SEED        = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ───────── LOAD MODEL ────────────────────────────────────────────────────────\n",
    "print(f\"Loading {MODEL_NAME} …\")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "model = SteeringModel(base_model, [STEER_LAY], DEVICE)\n",
    "print(\"Model ready.\\n\")\n",
    "\n",
    "# ───────── PREPARE STOP IDS ─────────────────────────────────────────────────\n",
    "STOP_IDS = tok(\"Final answer:\", add_special_tokens=False).input_ids\n",
    "\n",
    "# ───────── HELPERS ──────────────────────────────────────────────────────────\n",
    "def kl_div(p, q):\n",
    "    return F.kl_div(\n",
    "        F.log_softmax(p, dim=-1),\n",
    "        F.softmax(q, dim=-1),\n",
    "        reduction=\"batchmean\",\n",
    "    ).item()\n",
    "\n",
    "def numeric_match(text, gold):\n",
    "    m = re.search(r'([-+]?\\d+(?:\\.\\d+)?)\\s*$', text)\n",
    "    if not m:\n",
    "        return False\n",
    "    try:\n",
    "        return math.isclose(float(m.group(1)), float(eval(gold)), rel_tol=1e-3)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def sample_next(logits, temperature=0.7, top_p=0.9):\n",
    "    # nucleus sampling with temperature\n",
    "    logits = logits / temperature\n",
    "    sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
    "    probs = torch.softmax(sorted_logits, dim=-1)\n",
    "    cum_probs = torch.cumsum(probs, dim=-1)\n",
    "    mask = cum_probs > top_p\n",
    "    mask[..., 0] = False\n",
    "    probs = probs.masked_fill(mask, 0.0)\n",
    "    probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "    choice = torch.multinomial(probs, num_samples=1)\n",
    "    return sorted_idx.gather(-1, choice)\n",
    "\n",
    "@torch.no_grad()\n",
    "def stream(prompt, max_len):\n",
    "    \"\"\"\n",
    "    Nucleus-sampled stream with trimmed KV-cache.\n",
    "    Returns full ids (prompt+gen) and per-step logits list.\n",
    "    \"\"\"\n",
    "    ids = tok(prompt, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "    past, logs = None, []\n",
    "    for _ in range(max_len):\n",
    "        out = model(\n",
    "            input_ids=ids[:, -1:], \n",
    "            past_key_values=past,\n",
    "            use_cache=True,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "        past = tuple(\n",
    "            (k[..., -K_WIN:, :].contiguous(), \n",
    "             v[..., -K_WIN:, :].contiguous())\n",
    "            for k, v in out.past_key_values\n",
    "        )\n",
    "        logits = out.logits[:, -1, :]\n",
    "        logs.append(logits.detach())\n",
    "        nxt = sample_next(logits)\n",
    "        token_id = nxt.item()\n",
    "        ids = torch.cat([ids, nxt], dim=-1)\n",
    "        if token_id in STOP_IDS or token_id == tok.eos_token_id:\n",
    "            break\n",
    "    return ids.squeeze(), logs\n",
    "\n",
    "# ───────── LOAD DATASET ─────────────────────────────────────────────────────\n",
    "gsm         = load_dataset(\"gsm8k\", \"main\")[\"test\"].shuffle(seed=SEED)\n",
    "train_rows  = gsm.select(range(50))   # use first 50 for vector build\n",
    "eval_rows   = gsm.select(range(50, 60))  # next 10 for quick eval\n",
    "\n",
    "# ───────── BUILD RAST VECTOR ────────────────────────────────────────────────\n",
    "def rs_add(pool, vec):\n",
    "    if len(pool) < MAX_POOL:\n",
    "        pool.append(vec)\n",
    "    else:\n",
    "        j = random.randrange(len(pool) + 1)\n",
    "        if j < MAX_POOL:\n",
    "            pool[j] = vec\n",
    "\n",
    "hi_vecs, lo_vecs = [], []\n",
    "print(\"Building RAST vector …\")\n",
    "for row in tqdm(train_rows):\n",
    "    prompt = f\"Question: {row['question']}.{SUFFIX}\"\n",
    "    ids, logs = stream(prompt, GEN_LIMIT)\n",
    "    out = model(\n",
    "        input_ids=ids.unsqueeze(0).to(DEVICE),\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    hs = out.hidden_states[STEER_LAY + 1][0]  # layer index +1 for embeddings layer\n",
    "    offset = len(ids) - len(logs)\n",
    "    for j in range(K_WIN, len(logs)):\n",
    "        dkl = kl_div(logs[j], logs[j - K_WIN])\n",
    "        vec = hs[offset + j].detach().cpu().numpy()\n",
    "        rs_add(hi_vecs if dkl >= DKL_THR else lo_vecs, vec)\n",
    "\n",
    "print(f\"  hi:{len(hi_vecs)}  lo:{len(lo_vecs)} (capped {MAX_POOL})\")\n",
    "vec_dir  = (np.mean(hi_vecs, axis=0) - np.mean(lo_vecs, axis=0)).astype(np.float32)\n",
    "rast_vec = SteeringVector({STEER_LAY: vec_dir})\n",
    "print(\"Vector ready.\\n\")\n",
    "\n",
    "# ───────── RAST-GENERATION ─────────────────────────────────────────────────\n",
    "@torch.no_grad()\n",
    "def rast_generate(prompt, vec):\n",
    "    ids  = tok(prompt, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "    past, buf = None, deque(maxlen=K_WIN + 1)\n",
    "    model.set_steering(vec, coeff=0.0)\n",
    "    for _ in range(MAX_TOKENS):\n",
    "        out = model(\n",
    "            input_ids=ids[:, -1:], \n",
    "            past_key_values=past,\n",
    "            use_cache=True,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "        past = tuple(\n",
    "            (k[..., -K_WIN:, :].contiguous(), \n",
    "             v[..., -K_WIN:, :].contiguous())\n",
    "            for k, v in out.past_key_values\n",
    "        )\n",
    "        logits = out.logits[:, -1, :]\n",
    "        buf.append(logits.detach())\n",
    "        if len(buf) > K_WIN and kl_div(logits, buf[0]) < DKL_THR:\n",
    "            model.coeff = ALPHA_HI\n",
    "        else:\n",
    "            model.coeff = 0.0\n",
    "        nxt = sample_next(logits)\n",
    "        token_id = nxt.item()\n",
    "        ids = torch.cat([ids, nxt], dim=-1)\n",
    "        if token_id in STOP_IDS or token_id == tok.eos_token_id:\n",
    "            break\n",
    "    model.reset_steering()\n",
    "    return ids.squeeze()\n",
    "\n",
    "# ───────── EVALUATION ───────────────────────────────────────────────────────\n",
    "print(f\"Evaluating on {len(eval_rows)} problems …\")\n",
    "tok_base, tok_rast, acc_base, acc_rast = [], [], [], []\n",
    "base_txts, rast_txts = [], []\n",
    "\n",
    "for row in tqdm(eval_rows):\n",
    "    prompt    = f\"Question: {row['question']}.{SUFFIX}\"\n",
    "    base_ids, _ = stream(prompt, MAX_TOKENS)\n",
    "    rast_ids    = rast_generate(prompt, rast_vec)\n",
    "\n",
    "    base_txt = tok.decode(base_ids, skip_special_tokens=True)\n",
    "    rast_txt = tok.decode(rast_ids, skip_special_tokens=True)\n",
    "    \n",
    "    base_txts.append(base_txt)\n",
    "    rast_txts.append(rast_txts)\n",
    "\n",
    "    tok_base.append(base_ids.numel())\n",
    "    tok_rast.append(rast_ids.numel())\n",
    "    \n",
    "    acc_base.append(numeric_match(base_txt, row[\"answer\"]))\n",
    "    acc_rast.append(numeric_match(rast_txt, row[\"answer\"]))\n",
    "\n",
    "print(\"──────── RESULTS ────────\")\n",
    "print(f\"Mean tokens baseline : {np.mean(tok_base):.1f}\")\n",
    "print(f\"Mean tokens RAST     : {np.mean(tok_rast):.1f}\")\n",
    "print(f\"Token saving         : {100*(np.mean(tok_base)-np.mean(tok_rast))/np.mean(tok_base):.1f}%\")\n",
    "print(f\"Accuracy baseline    : {np.mean(acc_base):.3f}\")\n",
    "print(f\"Accuracy RAST        : {np.mean(acc_rast):.3f}\")\n",
    "print(\"────────────────────────\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

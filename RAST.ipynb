{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb38626-aa67-47ae-8add-0b1ccf7d5a7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate scikit-learn #matplotlib seaborn datasets scikit-learn modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121319c0-e601-491e-b4d2-0477a10a3529",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Stub' object has no attribute 'mount'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      5\u001b[39m image = modal.Image.debian_slim().pip_install(\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtransformers\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Mount your steerit package (assuming it's local)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mstub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmount\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m./steerit\u001b[39m\u001b[33m\"\u001b[39m, remote_path=\u001b[33m\"\u001b[39m\u001b[33m/root/steerit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;129m@stub\u001b[39m.function(\n\u001b[32m     17\u001b[39m     image=image,\n\u001b[32m     18\u001b[39m     gpu=\u001b[33m\"\u001b[39m\u001b[33mA10G\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_rast\u001b[39m():\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Stub' object has no attribute 'mount'"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "\n",
    "stub = modal.Stub(\"rast-modal-runner\")\n",
    "\n",
    "image = modal.Image.debian_slim().pip_install(\n",
    "    \"torch\",\n",
    "    \"transformers\",\n",
    "    \"datasets\",\n",
    "    \"tqdm\",\n",
    "    \"numpy\",\n",
    ")\n",
    "\n",
    "# Mount your steerit package (assuming it's local)\n",
    "stub.mount(\"./steerit\", remote_path=\"/root/steerit\")\n",
    "\n",
    "@stub.function(\n",
    "    image=image,\n",
    "    gpu=\"A10G\",\n",
    "    timeout=1800,\n",
    "    secrets=[modal.Secret.from_name(\"huggingface\")],\n",
    ")\n",
    "def run_rast():\n",
    "    import math, random, warnings, re\n",
    "    from collections import deque\n",
    "\n",
    "    import numpy as np\n",
    "    import torch, torch.nn.functional as F\n",
    "    from tqdm import tqdm\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    from steerit.steerit import SteeringModel, SteeringVector\n",
    "\n",
    "    MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    HF_TOKEN   = os.getenv(\"HF_TOKEN\", None)\n",
    "    DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    DTYPE      = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "    STEER_LAY  = 20\n",
    "    K_WIN      = 16\n",
    "    DKL_THR    = 0.15\n",
    "    ALPHA_HI   = 7.0\n",
    "    GEN_LIMIT  = 512\n",
    "    MAX_TOKENS = 4096\n",
    "    MAX_POOL   = 4000\n",
    "    SUFFIX     = \" Answer step by step and end with: Final answer:\"\n",
    "    SEED       = 42\n",
    "\n",
    "    random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "        use_auth_token=HF_TOKEN,\n",
    "    )\n",
    "    model = SteeringModel(base_model, [STEER_LAY], DEVICE)\n",
    "    STOP_IDS = tok(\"Final answer:\", add_special_tokens=False).input_ids\n",
    "\n",
    "    def kl_div(a, b):\n",
    "        p, q = torch.softmax(a, -1), torch.softmax(b, -1)\n",
    "        return (p * (p.log() - q.log())).sum(-1).item()\n",
    "\n",
    "    def numeric_match(text, gold):\n",
    "        m = re.search(r'([-+]?\\d+(?:\\.\\d+)?)\\s*$', text)\n",
    "        if not m: return False\n",
    "        try: return math.isclose(float(m.group(1)), float(eval(gold)), 1e-3)\n",
    "        except: return False\n",
    "\n",
    "    def sample_next_cpu(logits_gpu, prev_ids, temperature=0.9, top_p=0.9, eps=1e-12):\n",
    "        dev = logits_gpu.device\n",
    "        logits = (logits_gpu.float() / temperature).cpu().numpy()[0].astype(np.float64)\n",
    "        logits = np.clip(logits, -80.0, 80.0)\n",
    "        probs = np.exp(logits - logits.max()); probs /= probs.sum()\n",
    "\n",
    "        idx = np.argsort(probs)[::-1]; cum = np.cumsum(probs[idx])\n",
    "        keep = cum <= top_p; keep[0] = True\n",
    "        mask = np.zeros_like(probs, dtype=bool); mask[idx[keep]] = True\n",
    "        probs = probs * mask + eps; probs /= probs.sum()\n",
    "\n",
    "        if not np.isfinite(probs).all(): probs[:] = 1.0 / len(probs)\n",
    "        for _ in range(10):\n",
    "            tok = np.random.choice(len(probs), p=probs)\n",
    "            if prev_ids.size(1) == 0 or tok != prev_ids[0, -1].item(): break\n",
    "        return torch.tensor([[tok]], device=dev, dtype=torch.long)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def stream(prompt, max_len, model, tok, k_win=None):\n",
    "        ids = tok(prompt, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "        past, logs = None, []\n",
    "        for _ in range(max_len):\n",
    "            inp = ids if past is None else ids[:, -1:]\n",
    "            out = model(input_ids=inp, past_key_values=past, use_cache=True)\n",
    "            past = out.past_key_values\n",
    "            if k_win:\n",
    "                past = tuple((k[..., -k_win:, :], v[..., -k_win:, :]) for k, v in past)\n",
    "            logits = out.logits[:, -1, :]\n",
    "            logs.append(logits.detach())\n",
    "            nxt = sample_next_cpu(logits, ids)\n",
    "            ids = torch.cat([ids, nxt], -1)\n",
    "            if nxt.item() in STOP_IDS or nxt.item() == tok.eos_token_id:\n",
    "                break\n",
    "        return ids.squeeze(0), logs\n",
    "\n",
    "    gsm = load_dataset(\"gsm8k\", \"main\")['test'].shuffle(SEED)\n",
    "    train_rows = gsm.select(range(50))\n",
    "    eval_rows = gsm.select(range(50, 60))\n",
    "\n",
    "    def reservoir(pool, vec):\n",
    "        if len(pool) < MAX_POOL: pool.append(vec)\n",
    "        else:\n",
    "            j = random.randrange(len(pool)+1)\n",
    "            if j < MAX_POOL: pool[j] = vec\n",
    "\n",
    "    hi, lo = [], []\n",
    "    for row in tqdm(train_rows):\n",
    "        prompt = f\"Question: {row['question']}.{SUFFIX}\"\n",
    "        ids, logs = stream(prompt, GEN_LIMIT, base_model, tok, k_win=K_WIN)\n",
    "        hs = model(input_ids=ids.unsqueeze(0).to(DEVICE), output_hidden_states=True).hidden_states[STEER_LAY][0]\n",
    "        off = len(ids) - len(logs)\n",
    "        for j in range(K_WIN, len(logs)):\n",
    "            vec = hs[off+j].detach().cpu().numpy()\n",
    "            reservoir(hi if kl_div(logs[j], logs[j-K_WIN]) >= DKL_THR else lo, vec)\n",
    "\n",
    "    vec_dir = np.mean(hi, 0) - np.mean(lo, 0)\n",
    "    vec_dir /= np.linalg.norm(vec_dir) + 1e-9\n",
    "    rast_vec = SteeringVector({STEER_LAY: vec_dir.astype(np.float32)})\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def rast_generate(prompt, steer_vec, model, tok, max_tokens=256):\n",
    "        ids = tok(prompt, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "        past, buf = None, deque(maxlen=K_WIN+1)\n",
    "        model.set_steering(steer_vec, coeff=0.0)\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            inp = ids if past is None else ids[:, -1:]\n",
    "            out = model(input_ids=inp, past_key_values=past, use_cache=True)\n",
    "            past = out.past_key_values\n",
    "            past = tuple((k[..., -K_WIN:, :], v[..., -K_WIN:, :]) for k, v in past)\n",
    "            logits = out.logits[:, -1, :]\n",
    "            buf.append(logits.detach())\n",
    "            model.coeff = ALPHA_HI if (len(buf) > K_WIN and kl_div(logits, buf[0]) < DKL_THR) else 0.0\n",
    "            nxt = sample_next_cpu(logits, ids)\n",
    "            ids = torch.cat([ids, nxt], -1)\n",
    "            if nxt.item() in STOP_IDS or nxt.item() == tok.eos_token_id:\n",
    "                break\n",
    "\n",
    "        model.reset_steering()\n",
    "        return ids.squeeze(0)\n",
    "\n",
    "    tok_b, tok_r, acc_b, acc_r = [], [], [], []\n",
    "    for row in tqdm(eval_rows):\n",
    "        prompt = f\"Question: {row['question']}.{SUFFIX}\"\n",
    "        base_ids, _ = stream(prompt, MAX_TOKENS, base_model, tok, k_win=K_WIN)\n",
    "        rast_ids = rast_generate(prompt, rast_vec, model, tok, max_tokens=MAX_TOKENS)\n",
    "\n",
    "        base_txt = tok.decode(base_ids, skip_special_tokens=True)\n",
    "        rast_txt = tok.decode(rast_ids, skip_special_tokens=True)\n",
    "\n",
    "        tok_b.append(base_ids.numel()); tok_r.append(rast_ids.numel())\n",
    "        acc_b.append(numeric_match(base_txt, row[\"answer\"]))\n",
    "        acc_r.append(numeric_match(rast_txt, row[\"answer\"]))\n",
    "\n",
    "    print(\"\\n──────── RESULTS ────────\")\n",
    "    print(f\"Mean tokens baseline : {np.mean(tok_b):.1f}\")\n",
    "    print(f\"Mean tokens RAST     : {np.mean(tok_r):.1f}\")\n",
    "    print(f\"Token saving         : {100*(np.mean(tok_b)-np.mean(tok_r))/np.mean(tok_b):.1f}%\")\n",
    "    print(f\"Accuracy baseline    : {np.mean(acc_b):.3f}\")\n",
    "    print(f\"Accuracy RAST        : {np.mean(acc_r):.3f}\")\n",
    "    print(\"────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e5cbb-f3e8-42d9-9148-2110e1c2a52d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

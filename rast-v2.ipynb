{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db90be3-9913-4b5c-9f95-c91a9808fc6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install repeng accelerate datasets matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9a04e8d3-b8f9-476e-9fe5-848e4b2da034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"To copy construct from a tensor\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0c3f45-3813-4d48-8f2c-9020b5695b6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RASPID with dynamic PID-steering:\n",
    "- Train chunk-level classifier and control vector on the first 1000 labeled chains\n",
    "- Hold out the last 200 chains for final GSM8K evaluation only\n",
    "\"\"\"\n",
    "\n",
    "import os, re, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from repeng import ControlModel\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ─── 1) CONFIG & LOAD ──────────────────────────────────────────────────────\n",
    "\n",
    "MODEL_NAME    = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE         = torch.float32\n",
    "\n",
    "LABELED_CSV   = \"gsm8k_chains_labeled_with_tokens.csv\"\n",
    "CTRL_VEC_PATH = \"ctrl_vector.pt\"\n",
    "\n",
    "# classifier hyperparams\n",
    "EMB_LAYER     = 20\n",
    "CHUNK_SIZES   = [16,24]\n",
    "BATCH_SIZE    = 32\n",
    "FLUFF_STAR    = 0.5   # target probability for “redundant”\n",
    "\n",
    "# PID steering hyperparams\n",
    "INIT_FREE     = 40\n",
    "STEER_WINDOW  = 60\n",
    "KP, KI, KD    = 0.05, 0.001, 0.001\n",
    "MAX_I, DERIV  = 0.20, 0.01\n",
    "MAX_ALPHA     = 0.40\n",
    "BASE_TEMP     = 0.70\n",
    "STEER_TEMP    = 0.20\n",
    "MAX_REPEAT    = 8\n",
    "\n",
    "# load tokenizer & models\n",
    "tokenizer    = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "base_model   = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE==\"cuda\" else None\n",
    ").eval()\n",
    "control_model = ControlModel(base_model, [EMB_LAYER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8971b2b-e39d-4728-8b9f-6ac4c64568c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ─── 2) SPLIT LABELED DATA ─────────────────────────────────────────────────\n",
    "\n",
    "df_all = pd.read_csv(LABELED_CSV)\n",
    "\n",
    "# first 1000 for training classifier & control vector\n",
    "df_ctrl = df_all.iloc[:1000]\n",
    "# last 200 reserved for later (but not used to train classifier)\n",
    "df_eval = df_all.iloc[1000:1200]\n",
    "\n",
    "required_ctrl  = df_ctrl[\"required_thoughts\"].fillna(\"\")\n",
    "redundant_ctrl = df_ctrl[\"redundant_thoughts\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9af953e-37c6-4003-a947-fd32cd6fa083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7987150481fe4f6f8d8a590a8380f08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd5f130f05b4f2fa622a7b552d6daac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training clf @ cs=16:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size=16 → val_acc=0.291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ab399847c744b0b54e526d78ca9294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efb5950b4e34ab89eb12f17a4f1bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training clf @ cs=24:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size=24 → val_acc=0.814\n",
      "✔ Selected chunk_size=24, val_acc=0.814\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Disable HuggingFace tokenizer parallel warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class ChunkDataset(Dataset):\n",
    "    def __init__(self, texts, label, cs):\n",
    "        self.cs = cs\n",
    "        self.chunks = []\n",
    "        self.labels = []\n",
    "        for t in texts:\n",
    "            tok_ids = tokenizer.encode(t, add_special_tokens=False)\n",
    "            for i in range(0, len(tok_ids) - cs + 1, cs):\n",
    "                self.chunks.append(tok_ids[i : i + cs])\n",
    "                self.labels.append(label)\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.chunks[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, labels = zip(*batch)\n",
    "    # pad on CPU so pin_memory works\n",
    "    seqs = [torch.tensor(ids, dtype=torch.long) for ids in input_ids]\n",
    "    padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        seqs, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    attention_mask = (padded != tokenizer.pad_token_id).long()\n",
    "    return {\"input_ids\": padded, \"attention_mask\": attention_mask}, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "best_cs, best_acc, best_clf = None, 0.0, None\n",
    "\n",
    "for cs in CHUNK_SIZES:\n",
    "    # build datasets\n",
    "    ds0 = ChunkDataset(required_ctrl, 0, cs)\n",
    "    ds1 = ChunkDataset(redundant_ctrl, 1, cs)\n",
    "    loader = DataLoader(\n",
    "        ConcatDataset([ds0, ds1]),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,  # no multiprocessing to avoid fork issues\n",
    "    )\n",
    "\n",
    "    # embed all chunks\n",
    "    feats, labels = [], []\n",
    "    base_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_tokens, batch_labels in tqdm(loader):\n",
    "            # move to GPU\n",
    "            batch_tokens = {\n",
    "                k: v.to(DEVICE, non_blocking=True)\n",
    "                for k, v in batch_tokens.items()\n",
    "            }\n",
    "            out = base_model(**batch_tokens, output_hidden_states=True)\n",
    "            h = out.hidden_states[EMB_LAYER].mean(1)\n",
    "            feats.append(h.cpu().numpy())\n",
    "            labels.append(batch_labels.numpy())\n",
    "    X = np.vstack(feats)\n",
    "    y = np.concatenate(labels)\n",
    "\n",
    "    # train/validation split\n",
    "    Xtr, Xval, ytr, yval = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # SGD training with tqdm progress\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"log_loss\", random_state=42, warm_start=True, max_iter=1, tol=None\n",
    "    )\n",
    "    prev_coef = None\n",
    "    pbar = tqdm(range(1000), desc=f\"Training clf @ cs={cs}\", leave=False)\n",
    "    for _ in pbar:\n",
    "        clf.fit(Xtr, ytr)\n",
    "        coef = clf.coef_\n",
    "        if prev_coef is not None:\n",
    "            delta = np.max(np.abs(coef - prev_coef))\n",
    "            pbar.set_postfix(delta=delta)\n",
    "            if delta < 1e-3:\n",
    "                break\n",
    "        prev_coef = coef.copy()\n",
    "    pbar.close()\n",
    "\n",
    "    acc = accuracy_score(yval, clf.predict(Xval))\n",
    "    print(f\"chunk_size={cs} → val_acc={acc:.3f}\")\n",
    "    if acc > best_acc:\n",
    "        best_cs, best_acc, best_clf = cs, acc, clf\n",
    "\n",
    "print(f\"✔ Selected chunk_size={best_cs}, val_acc={best_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f8eb6e6-1984-4c4b-b256-104daf0ffbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed9b27ab5e04615b3ccb6619daa46f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f431c0a18e46f8ad96f85088a4f3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved control vector\n"
     ]
    }
   ],
   "source": [
    "# ─── 4) BUILD CONTROL VECTOR FROM CTRL SET ────────────────────────────────\n",
    "\n",
    "def mean_hidden(texts):\n",
    "    vs = []\n",
    "    for t in tqdm(texts):\n",
    "        toks = tokenizer(t, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
    "        with torch.inference_mode():\n",
    "            h = base_model(**toks, output_hidden_states=True).hidden_states[EMB_LAYER][0]\n",
    "        vs.append(h.mean(0).cpu())\n",
    "    return torch.stack(vs).mean(0)\n",
    "\n",
    "v_req = mean_hidden(required_ctrl)\n",
    "v_red = mean_hidden(redundant_ctrl)\n",
    "ctrl_vec = {EMB_LAYER: (v_req - v_red).to(DEVICE)}\n",
    "torch.save(ctrl_vec, CTRL_VEC_PATH)\n",
    "print(\"✅ Saved control vector\")\n",
    "\n",
    "# ─── 5) GENERATION ROUTINES ───────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e561d1ef-8610-46a6-9e1e-d4c8a39850bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from repeng import ControlVector\n",
    "\n",
    "# model_type is a short string identifying your model, e.g. \"qwen\" or whatever base_model.config.model_type gives\n",
    "model_type = base_model.config.model_type  # e.g. \"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "ctrl_vec = ControlVector(\n",
    "    model_type = model_type,\n",
    "    directions={EMB_LAYER: (v_req - v_red).to(DEVICE)}\n",
    ")\n",
    "\n",
    "# now you can save it\n",
    "torch.save(ctrl_vec, \"ctrl_vector.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da85956c-af77-4c2c-9fb4-5fd728b2d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "688d6cf0-2047-41b2-8165-c16e418e9a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_baseline(prompt, max_new_tokens=MAX_TOKENS):\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = base_model.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True, temperature=0.6,\n",
    "        top_p=0.9, repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    toks = out.shape[1] - inp.input_ids.shape[1]\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True), toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd647c6e-0d84-45b0-8441-61d933b00e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tuned hyper-params\n",
    "INIT_FREE      = 80          # let the model reason first\n",
    "STEER_WINDOW   = 60\n",
    "BASE_TEMP      = 0.60        # same as baseline\n",
    "STEER_TEMP     = 0.30\n",
    "STEER_MARGIN   = 0.20        # p_red must exceed 0.5+0.2\n",
    "MAX_ALPHA      = 0.40\n",
    "MAX_RAW        = 50.0        # clamp for exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc595974-1a46-4b1e-b5a4-273f11afcc56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_raspid(prompt, max_new_tokens=MAX_TOKENS, debug=True):\n",
    "    \"\"\"\n",
    "    Fixed RASPID generation function that properly applies the control vector\n",
    "    and handles numerical stability issues.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"\\n=== RASPID GENERATION ===\")\n",
    "        print(f\"Original prompt: '{prompt}'\")\n",
    "        print(f\"MAX_TOKENS: {MAX_TOKENS}, INIT_FREE: {INIT_FREE}, STEER_WINDOW: {STEER_WINDOW}\")\n",
    "        print(f\"Temperatures: BASE_TEMP: {BASE_TEMP}, STEER_TEMP: {STEER_TEMP}\")\n",
    "        print(f\"PID: KP={KP}, KI={KI}, KD={KD}, MAX_I={MAX_I}, MAX_ALPHA={MAX_ALPHA}\")\n",
    "        print(f\"FLUFF_STAR: {FLUFF_STAR}, EMB_LAYER: {EMB_LAYER}\")\n",
    "    \n",
    "    # For structured control vectors (like ControlVector with directions dict),\n",
    "    # we should leave the structure intact\n",
    "    ctrl_vec_normalized = ctrl_vec\n",
    "\n",
    "    # Setup generation\n",
    "    stop_re = re.compile(r\"\\\\boxed\\{[^{}]{1,12}\\}\")\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE).input_ids[0]\n",
    "    out_ids = ids.clone()\n",
    "    past = None\n",
    "    alpha = I = D = prev_err = 0.0\n",
    "    chunk_h = None\n",
    "    tok_in_chunk = 0\n",
    "    steering = False\n",
    "    steer_start = 0\n",
    "    last_tok = None\n",
    "    rep_ctr = 0\n",
    "    generated_text = \"\"\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n--- RASPID TRACE ---\")\n",
    "        print(\"step | on | p_red |  err  |   α   |   I   |   D   | temp | token\")\n",
    "    \n",
    "    pbar = tqdm(range(max_new_tokens), desc=\"RASPID gen\", leave=False)\n",
    "    for step in pbar:\n",
    "        gen_len = out_ids.size(0) - ids.size(0)\n",
    "        \n",
    "        # Check if steering should be activated/deactivated\n",
    "        if not steering and gen_len >= INIT_FREE:\n",
    "            steering, steer_start = True, gen_len\n",
    "            if debug:\n",
    "                print(f\"[INFO] Activating steering at gen_len={gen_len}\")\n",
    "        if steering and gen_len - steer_start > STEER_WINDOW:\n",
    "            if debug:\n",
    "                print(f\"[INFO] Deactivating steering at gen_len={gen_len}\")\n",
    "            steering, alpha, I, D = False, 0.0, 0.0, 0.0\n",
    "        \n",
    "        # Get coefficient and apply control\n",
    "        coeff = alpha if steering else 0.0\n",
    "        control_model.set_control(ctrl_vec_normalized, coeff=coeff)\n",
    "        \n",
    "        # CRITICAL FIX: Process the entire prompt on the first step\n",
    "        if gen_len == 0 and step == 0:\n",
    "            # First step - process the entire prompt\n",
    "            out = control_model(\n",
    "                input_ids=out_ids.unsqueeze(0),  # Use the full prompt\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        else:\n",
    "            # Subsequent steps - process only the new token with the cached state\n",
    "            out = control_model(\n",
    "                input_ids=out_ids[-1:].unsqueeze(0),  # Only the last token\n",
    "                past_key_values=past,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        \n",
    "        past, logits = out.past_key_values, out.logits[0, -1]\n",
    "        h_last = out.hidden_states[EMB_LAYER][0, -1]\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if torch.isnan(logits).any():\n",
    "            logits = torch.nan_to_num(logits)\n",
    "        if torch.isnan(h_last).any():\n",
    "            h_last = torch.nan_to_num(h_last)\n",
    "        \n",
    "        # Check for token repetition\n",
    "        tok = out_ids[-1].item()\n",
    "        if tok == last_tok:\n",
    "            rep_ctr += 1\n",
    "            if rep_ctr >= MAX_REPEAT:\n",
    "                if debug:\n",
    "                    print(f\"[INFO] Hit MAX_REPEAT={MAX_REPEAT}, stopping generation\")\n",
    "                break\n",
    "        else:\n",
    "            rep_ctr, last_tok = 0, tok\n",
    "        \n",
    "        # Normalize hidden states for stability\n",
    "        h_last_norm = h_last / (torch.norm(h_last) + 1e-8)\n",
    "        \n",
    "        # Track hidden states with normalized values\n",
    "        chunk_h = h_last_norm if chunk_h is None else chunk_h + h_last_norm\n",
    "        tok_in_chunk += 1\n",
    "        \n",
    "        # Classifier and PID controller\n",
    "        p_red = err = 0.0\n",
    "        if tok_in_chunk >= best_cs:\n",
    "            try:\n",
    "                # Use normalized chunk_h for classifier\n",
    "                classifier_input = (chunk_h / best_cs).cpu().unsqueeze(0).numpy()\n",
    "                \n",
    "                # Check for NaN values\n",
    "                if np.isnan(classifier_input).any():\n",
    "                    classifier_input = np.nan_to_num(classifier_input)\n",
    "                \n",
    "                # Get raw classifier output\n",
    "                raw = best_clf.decision_function(classifier_input)[0]\n",
    "                \n",
    "                # Scale down extreme classifier values\n",
    "                if abs(raw) > 50.0:\n",
    "                    scaled_raw = 50.0 * (np.sign(raw) * np.log(1 + abs(raw) / 50.0) / np.log(1 + abs(raw) / 50.0 * 20))\n",
    "                    if debug:\n",
    "                        print(f\"[INFO] Scaling down extreme classifier value: {raw:.2f} -> {scaled_raw:.2f}\")\n",
    "                    raw = scaled_raw\n",
    "                else:\n",
    "                    raw = max(-50.0, min(50.0, raw))\n",
    "                \n",
    "                p_red = 1.0 / (1.0 + math.exp(-raw))\n",
    "                \n",
    "                if p_red > FLUFF_STAR + 0.20:\n",
    "                    err = p_red - FLUFF_STAR\n",
    "                    \n",
    "                    # Update PID controller\n",
    "                    I = max(-MAX_I, min(MAX_I, I + KI * err))\n",
    "                    D = KD * (err - prev_err) + (1 - KD) * D\n",
    "                    prev_err = err\n",
    "                    alpha = max(0.0, min(MAX_ALPHA, alpha + KP * err + I + D))\n",
    "                \n",
    "                # Reset chunk\n",
    "                chunk_h = None\n",
    "                tok_in_chunk = 0\n",
    "                \n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"[ERROR] Error in classifier/PID section: {e}\")\n",
    "                chunk_h = h_last_norm\n",
    "                tok_in_chunk = 1\n",
    "        \n",
    "        # Temperature and sampling\n",
    "        temp = BASE_TEMP * (1 - coeff / MAX_ALPHA) + STEER_TEMP * (coeff / MAX_ALPHA)\n",
    "        \n",
    "        # Apply temperature and get probabilities\n",
    "        try:\n",
    "            logits_safe = logits.clamp(-100, 100)\n",
    "            probs = torch.softmax(logits_safe / temp, dim=-1)\n",
    "            \n",
    "            if torch.isnan(probs).any():\n",
    "                probs = torch.ones_like(probs) / probs.size(0)\n",
    "            \n",
    "            # Sample token\n",
    "            nxt = torch.multinomial(probs, 1).item()\n",
    "            token_str = tokenizer.decode([nxt], skip_special_tokens=True).replace(\"\\n\",\"\\\\n\")\n",
    "            \n",
    "            # Add token to output\n",
    "            generated_text += token_str\n",
    "            out_ids = torch.cat([out_ids, torch.tensor([nxt], device=DEVICE)])\n",
    "            \n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"[ERROR] Error in sampling: {e}\")\n",
    "            # Fallback to argmax sampling\n",
    "            nxt = torch.argmax(logits).item()\n",
    "            token_str = tokenizer.decode([nxt], skip_special_tokens=True).replace(\"\\n\",\"\\\\n\")\n",
    "            generated_text += token_str\n",
    "            out_ids = torch.cat([out_ids, torch.tensor([nxt], device=DEVICE)])\n",
    "        \n",
    "        # Print trace line if in debug mode\n",
    "        if debug:\n",
    "            print(f\"{gen_len:4d} | {int(steering)} | {p_red:5.3f} | {err:5.3f} | \"\n",
    "                  f\"{alpha:5.3f} | {I:5.3f} | {D:5.3f} | {temp:5.3f} | '{token_str}'\")\n",
    "        \n",
    "        # Check stop condition\n",
    "        if stop_re.search(generated_text) or \"Final answer:\" in generated_text:\n",
    "            if debug:\n",
    "                print(f\"[INFO] Stop condition met, ending generation\")\n",
    "            break\n",
    "    \n",
    "    pbar.close()\n",
    "    if debug:\n",
    "        print(\"--- END TRACE ---\\n\")\n",
    "        print(f\"=== GENERATION RESULT ===\")\n",
    "        print(f\"Total tokens generated: {out_ids.size(0) - ids.size(0)}\")\n",
    "        print(f\"Final text: {tokenizer.decode(out_ids, skip_special_tokens=True)}\")\n",
    "    \n",
    "    return tokenizer.decode(out_ids, skip_special_tokens=True), out_ids.size(0) - ids.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9aaba8f8-290c-40b4-9f6c-6d451920c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_raspid_fixed_v2(prompt, max_new_tokens=MAX_TOKENS, debug=True):\n",
    "    \"\"\"\n",
    "    Enhanced RASPID generation function with improved parameters for more effective steering.\n",
    "    \n",
    "    Key improvements:\n",
    "    1. Start steering earlier (reduced INIT_FREE)\n",
    "    2. Stronger steering effect (increased KP and MAX_ALPHA)\n",
    "    3. Consistent steering (longer STEER_WINDOW)\n",
    "    4. Better control application\n",
    "    \"\"\"\n",
    "    # Override default parameters with improved values\n",
    "    INIT_FREE_ORIG = INIT_FREE  # Store original value\n",
    "    STEER_WINDOW_ORIG = STEER_WINDOW  # Store original value\n",
    "    KP_ORIG = KP  # Store original value\n",
    "    MAX_ALPHA_ORIG = MAX_ALPHA  # Store original value\n",
    "    \n",
    "    # Override with enhanced parameters for better steering\n",
    "    INIT_FREE_ENHANCED = 15  # Start steering much earlier (was 80)\n",
    "    STEER_WINDOW_ENHANCED = 300  # Longer window to maintain steering (was 60)\n",
    "    KP_ENHANCED = 0.15  # Stronger proportional control (was 0.05)\n",
    "    MAX_ALPHA_ENHANCED = 1.0  # Allow stronger steering effect (was 0.4)\n",
    "    \n",
    "    # Use the enhanced parameters\n",
    "    INIT_FREE_USED = INIT_FREE_ENHANCED\n",
    "    STEER_WINDOW_USED = STEER_WINDOW_ENHANCED\n",
    "    KP_USED = KP_ENHANCED\n",
    "    MAX_ALPHA_USED = MAX_ALPHA_ENHANCED\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n=== ENHANCED RASPID GENERATION ===\")\n",
    "        print(f\"Original prompt: '{prompt}'\")\n",
    "        print(f\"MAX_TOKENS: {max_new_tokens}\")\n",
    "        print(f\"INIT_FREE: {INIT_FREE_USED} (was {INIT_FREE_ORIG}) - Start steering earlier\")\n",
    "        print(f\"STEER_WINDOW: {STEER_WINDOW_USED} (was {STEER_WINDOW_ORIG}) - More consistent steering\")\n",
    "        print(f\"KP: {KP_USED} (was {KP_ORIG}) - Stronger steering response\")\n",
    "        print(f\"MAX_ALPHA: {MAX_ALPHA_USED} (was {MAX_ALPHA_ORIG}) - Higher maximum steering\")\n",
    "        print(f\"Other parameters: KI={KI}, KD={KD}, MAX_I={MAX_I}\")\n",
    "        print(f\"BASE_TEMP: {BASE_TEMP}, STEER_TEMP: {STEER_TEMP}\")\n",
    "        print(f\"FLUFF_STAR: {FLUFF_STAR}, EMB_LAYER: {EMB_LAYER}\")\n",
    "    \n",
    "    # For structured control vectors (like ControlVector with directions dict),\n",
    "    # we should leave the structure intact\n",
    "    ctrl_vec_normalized = ctrl_vec\n",
    "\n",
    "    # Setup generation\n",
    "    stop_re = re.compile(r\"\\\\boxed\\{[^{}]{1,12}\\}\")\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE).input_ids[0]\n",
    "    out_ids = ids.clone()\n",
    "    past = None\n",
    "    alpha = I = D = prev_err = 0.0\n",
    "    chunk_h = None\n",
    "    tok_in_chunk = 0\n",
    "    steering = False\n",
    "    steer_start = 0\n",
    "    last_tok = None\n",
    "    rep_ctr = 0\n",
    "    generated_text = \"\"\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n--- RASPID TRACE ---\")\n",
    "        print(\"step | on | p_red |  err  |   α   |   I   |   D   | temp | token\")\n",
    "    \n",
    "    pbar = tqdm(range(max_new_tokens), desc=\"RASPID gen\", leave=False)\n",
    "    for step in pbar:\n",
    "        gen_len = out_ids.size(0) - ids.size(0)\n",
    "        \n",
    "        # Check if steering should be activated/deactivated\n",
    "        if not steering and gen_len >= INIT_FREE_USED:\n",
    "            steering, steer_start = True, gen_len\n",
    "            if debug:\n",
    "                print(f\"[INFO] Activating steering at gen_len={gen_len}\")\n",
    "        if steering and gen_len - steer_start > STEER_WINDOW_USED:\n",
    "            if debug:\n",
    "                print(f\"[INFO] Deactivating steering at gen_len={gen_len}\")\n",
    "            steering, alpha, I, D = False, 0.0, 0.0, 0.0\n",
    "        \n",
    "        # Get coefficient and apply control\n",
    "        coeff = alpha if steering else 0.0\n",
    "        \n",
    "        # Apply control - now using the correct coefficient\n",
    "        control_model.set_control(ctrl_vec_normalized, coeff=coeff)\n",
    "        \n",
    "        # CRITICAL FIX: Process the entire prompt on the first step\n",
    "        if gen_len == 0 and step == 0:\n",
    "            # First step - process the entire prompt\n",
    "            out = control_model(\n",
    "                input_ids=out_ids.unsqueeze(0),  # Use the full prompt\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        else:\n",
    "            # Subsequent steps - process only the new token with the cached state\n",
    "            out = control_model(\n",
    "                input_ids=out_ids[-1:].unsqueeze(0),  # Only the last token\n",
    "                past_key_values=past,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        \n",
    "        past, logits = out.past_key_values, out.logits[0, -1]\n",
    "        h_last = out.hidden_states[EMB_LAYER][0, -1]\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if torch.isnan(logits).any():\n",
    "            logits = torch.nan_to_num(logits)\n",
    "        if torch.isnan(h_last).any():\n",
    "            h_last = torch.nan_to_num(h_last)\n",
    "        \n",
    "        # Check for token repetition\n",
    "        tok = out_ids[-1].item()\n",
    "        if tok == last_tok:\n",
    "            rep_ctr += 1\n",
    "            if rep_ctr >= MAX_REPEAT:\n",
    "                if debug:\n",
    "                    print(f\"[INFO] Hit MAX_REPEAT={MAX_REPEAT}, stopping generation\")\n",
    "                break\n",
    "        else:\n",
    "            rep_ctr, last_tok = 0, tok\n",
    "        \n",
    "        # Normalize hidden states for stability\n",
    "        h_last_norm = h_last / (torch.norm(h_last) + 1e-8)\n",
    "        \n",
    "        # Track hidden states with normalized values\n",
    "        chunk_h = h_last_norm if chunk_h is None else chunk_h + h_last_norm\n",
    "        tok_in_chunk += 1\n",
    "        \n",
    "        # Classifier and PID controller\n",
    "        p_red = err = 0.0\n",
    "        if tok_in_chunk >= best_cs:\n",
    "            try:\n",
    "                # Use normalized chunk_h for classifier\n",
    "                classifier_input = (chunk_h / best_cs).cpu().unsqueeze(0).numpy()\n",
    "                \n",
    "                # Check for NaN values\n",
    "                if np.isnan(classifier_input).any():\n",
    "                    classifier_input = np.nan_to_num(classifier_input)\n",
    "                \n",
    "                # Get raw classifier output\n",
    "                raw = best_clf.decision_function(classifier_input)[0]\n",
    "                \n",
    "                # Scale down extreme classifier values\n",
    "                if abs(raw) > 50.0:\n",
    "                    scaled_raw = 50.0 * (np.sign(raw) * np.log(1 + abs(raw) / 50.0) / np.log(1 + abs(raw) / 50.0 * 20))\n",
    "                    if debug:\n",
    "                        print(f\"[INFO] Scaling down extreme classifier value: {raw:.2f} -> {scaled_raw:.2f}\")\n",
    "                    raw = scaled_raw\n",
    "                else:\n",
    "                    raw = max(-50.0, min(50.0, raw))\n",
    "                \n",
    "                p_red = 1.0 / (1.0 + math.exp(-raw))\n",
    "                \n",
    "                if p_red > FLUFF_STAR + 0.20:\n",
    "                    err = p_red - FLUFF_STAR\n",
    "                    \n",
    "                    # Update PID controller with enhanced KP\n",
    "                    I = max(-MAX_I, min(MAX_I, I + KI * err))\n",
    "                    D = KD * (err - prev_err) + (1 - KD) * D\n",
    "                    prev_err = err\n",
    "                    \n",
    "                    # Use enhanced KP for stronger steering\n",
    "                    alpha = max(0.0, min(MAX_ALPHA_USED, alpha + KP_USED * err + I + D))\n",
    "                \n",
    "                # Reset chunk\n",
    "                chunk_h = None\n",
    "                tok_in_chunk = 0\n",
    "                \n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"[ERROR] Error in classifier/PID section: {e}\")\n",
    "                chunk_h = h_last_norm\n",
    "                tok_in_chunk = 1\n",
    "        \n",
    "        # Temperature and sampling - updated for enhanced MAX_ALPHA\n",
    "        temp = BASE_TEMP * (1 - coeff / MAX_ALPHA_USED) + STEER_TEMP * (coeff / MAX_ALPHA_USED)\n",
    "        \n",
    "        # Apply temperature and get probabilities\n",
    "        try:\n",
    "            logits_safe = logits.clamp(-100, 100)\n",
    "            probs = torch.softmax(logits_safe / temp, dim=-1)\n",
    "            \n",
    "            if torch.isnan(probs).any():\n",
    "                probs = torch.ones_like(probs) / probs.size(0)\n",
    "            \n",
    "            # Sample token\n",
    "            nxt = torch.multinomial(probs, 1).item()\n",
    "            token_str = tokenizer.decode([nxt], skip_special_tokens=True).replace(\"\\n\",\"\\\\n\")\n",
    "            \n",
    "            # Add token to output\n",
    "            generated_text += token_str\n",
    "            out_ids = torch.cat([out_ids, torch.tensor([nxt], device=DEVICE)])\n",
    "            \n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"[ERROR] Error in sampling: {e}\")\n",
    "            # Fallback to argmax sampling\n",
    "            nxt = torch.argmax(logits).item()\n",
    "            token_str = tokenizer.decode([nxt], skip_special_tokens=True).replace(\"\\n\",\"\\\\n\")\n",
    "            generated_text += token_str\n",
    "            out_ids = torch.cat([out_ids, torch.tensor([nxt], device=DEVICE)])\n",
    "        \n",
    "        # Print trace line if in debug mode\n",
    "        if debug:\n",
    "            print(f\"{gen_len:4d} | {int(steering)} | {p_red:5.3f} | {err:5.3f} | \"\n",
    "                  f\"{alpha:5.3f} | {I:5.3f} | {D:5.3f} | {temp:5.3f} | '{token_str}'\")\n",
    "        \n",
    "        # Check stop condition\n",
    "        if stop_re.search(generated_text) or \"Final answer:\" in generated_text:\n",
    "            if debug:\n",
    "                print(f\"[INFO] Stop condition met, ending generation\")\n",
    "            break\n",
    "    \n",
    "    pbar.close()\n",
    "    if debug:\n",
    "        print(\"--- END TRACE ---\\n\")\n",
    "        print(f\"=== GENERATION RESULT ===\")\n",
    "        print(f\"Total tokens generated: {out_ids.size(0) - ids.size(0)}\")\n",
    "        print(f\"Final text: {tokenizer.decode(out_ids, skip_special_tokens=True)}\")\n",
    "    \n",
    "    return tokenizer.decode(out_ids, skip_special_tokens=True), out_ids.size(0) - ids.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ef07dca7-59dd-44f7-9873-6005cbf5f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_answer(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the content of the last \\boxed{} occurrence in the string.\n",
    "    This handles cases where the model might have multiple boxed answers,\n",
    "    ensuring we get the final one.\n",
    "    \n",
    "    Args:\n",
    "        s: The string to extract the answer from\n",
    "        \n",
    "    Returns:\n",
    "        The content inside the last \\boxed{} occurrence, or empty string if none found\n",
    "    \"\"\"\n",
    "    # Find all occurrences of \\boxed{...}\n",
    "    matches = list(re.finditer(r\"\\\\boxed\\{([^}]+)\\}\", s))\n",
    "    \n",
    "    # Return the last match, if any\n",
    "    if matches:\n",
    "        return matches[-1].group(1).strip()\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6f9571b1-b8ee-4198-b775-5072f1a066aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_gsm8k(n_probs, max_tokens, debug = False):\n",
    "    gsm = load_dataset(\"gsm8k\", \"main\")[\"test\"].select(range(1000,1000+n_probs))\n",
    "    rec = []\n",
    "    baseline_total = 0\n",
    "    raspid_total = 0\n",
    "    for ex in tqdm(gsm):\n",
    "        q   = ex[\"question\"].strip()\n",
    "        prompt = f\"{q}\\n\\nAnswer step by step and end with: Final answer: \\\\boxed{{numeric_value}}\"\n",
    "        r_txt,r_tok = generate_raspid_fixed_v2(prompt, max_tokens, debug = debug)\n",
    "        b_txt,b_tok = generate_baseline(prompt, max_tokens)\n",
    "        \n",
    "\n",
    "        rec.append({\n",
    "            \"reference_answer\":ex[\"answer\"],\n",
    "            \"baseline_correct\": norm_answer(b_txt),\n",
    "            \"raspid_correct\":  norm_answer(r_txt),\n",
    "            \"baseline_tokens\": b_tok,\n",
    "            \"raspid_tokens\":  r_tok,\n",
    "            \"baseline_txt\": b_txt,\n",
    "            \"raspid_txt\":  r_txt,\n",
    "\n",
    "        })\n",
    "        baseline_total += int(b_tok)\n",
    "        raspid_total += int(r_tok)\n",
    "        print(f'total-token-usage for baseline: {baseline_total} raspid: {raspid_total}')\n",
    "\n",
    "    df = pd.DataFrame(rec)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d333258-3570-4b0a-90d9-28730877042d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e619179a59346499f0f9afaa4a4f5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6a07c062654541b9ff416bac32f9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 1253 raspid: 1439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9189a9db2b23446ea000f2c494152443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = run_gsm8k(50, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdd806-f2a6-4b8c-aaf3-c265bc7b3440",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results_df_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c6aea4d2-393a-490e-8dc9-d58318a6aa58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442ace21f5294b81befee3804cdb163d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70c46e7e18748099c8efd8c64a24d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321e660ed55e4964bdb84667c02ea941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cceb903f811a48619274c91dd22f125e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d346b04103542ccadfcbc243199da90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c526ac0bb63c4943a8b42e3e335625fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699ba719ec074588922d237789d3de7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cd3906ef004e1591ecddae083ab48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fc91c09fa94fb58048f576c50d2bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c7d44e14bb48c18988846fe9268a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522fe47459524e39a819bf16eb0e0592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae9ac79429b4dd1b3f28a19450e0be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3cea113a924aa48e44061639d8e9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a273ccd6b454b5486d7d7ec6fb9379e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff40f4536f1741fca84cadf09aa46193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5260a71bb8247379474bb59f4bcc423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d760a9fc8f6a4d0380f6fdd29998d2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff600e16bad04773abddb184b4c8efc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e502f24a5cfb4e3c8412c4e2b7316dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68677a538a7542009ab49ecc1a97888b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a8a87514b8456489914e6909533ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "records = []\n",
    "for i in tqdm(range(10), desc='Eval runs'):\n",
    "    a = generate_baseline('what is square root of 256', 1024)\n",
    "    b = generate_raspid('what is square root of 256', 1024, False)\n",
    "    c = generate_raspid_fixed_v2('what is square root of 256', 1024, False)\n",
    "    records.append({\n",
    "        'run': i + 1,\n",
    "        'baseline_tokens': a[1],\n",
    "        'raspid_tokens': b[1],\n",
    "        'raspid_fixed_tokens': c[1],\n",
    "        'baseline_answer': norm_answer(a[0]),\n",
    "        'raspid_answer': norm_answer(b[0]),\n",
    "        'raspid_fixed_answer': norm_answer(c[0])\n",
    "\n",
    "    })\n",
    "\n",
    "records = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "57d363a6-21b2-4cb3-b130-bf8c911a3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_columns = [        \n",
    "    'baseline_tokens',\n",
    "    'raspid_tokens',\n",
    "    'raspid_fixed_tokens',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9edda720-fc3d-4669-b0aa-2dd68796f5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>baseline_tokens</th>\n",
       "      <th>raspid_tokens</th>\n",
       "      <th>raspid_fixed_tokens</th>\n",
       "      <th>baseline_answer</th>\n",
       "      <th>raspid_answer</th>\n",
       "      <th>raspid_fixed_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>69</td>\n",
       "      <td>408</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>486</td>\n",
       "      <td>340</td>\n",
       "      <td>287</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>282</td>\n",
       "      <td>249</td>\n",
       "      <td>224</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>253</td>\n",
       "      <td>378</td>\n",
       "      <td>442</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>409</td>\n",
       "      <td>1024</td>\n",
       "      <td>161</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>75</td>\n",
       "      <td>210</td>\n",
       "      <td>166</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>367</td>\n",
       "      <td>359</td>\n",
       "      <td>332</td>\n",
       "      <td>\\sqrt{256</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>450</td>\n",
       "      <td>205</td>\n",
       "      <td>275</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>81</td>\n",
       "      <td>539</td>\n",
       "      <td>198</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>311</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run  baseline_tokens  raspid_tokens  raspid_fixed_tokens baseline_answer  \\\n",
       "0    1              178             69                  408              16   \n",
       "1    2              486            340                  287              16   \n",
       "2    3              282            249                  224              16   \n",
       "3    4              253            378                  442              16   \n",
       "4    5              409           1024                  161              16   \n",
       "5    6               75            210                  166              16   \n",
       "6    7              367            359                  332       \\sqrt{256   \n",
       "7    8              450            205                  275              16   \n",
       "8    9               81            539                  198              16   \n",
       "9   10             1024           1024                  311                   \n",
       "\n",
       "  raspid_answer raspid_fixed_answer  \n",
       "0            16                  16  \n",
       "1            16                  16  \n",
       "2            16                  16  \n",
       "3            16                  16  \n",
       "4                                16  \n",
       "5            16                  16  \n",
       "6            16                  16  \n",
       "7            16                  16  \n",
       "8            16                  16  \n",
       "9                                16  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "96b530ca-c0c0-4dad-8153-d244a9ab2964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baseline_tokens        360.5\n",
       "raspid_tokens          439.7\n",
       "raspid_fixed_tokens    280.4\n",
       "dtype: float64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[token_columns].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2fdd4b0f-74aa-45a4-ae37-16b38a02dae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ENHANCED RASPID GENERATION ===\n",
      "Original prompt: 'what is square root of 81'\n",
      "MAX_TOKENS: 4096\n",
      "INIT_FREE: 15 (was 80) - Start steering earlier\n",
      "STEER_WINDOW: 300 (was 60) - More consistent steering\n",
      "KP: 0.15 (was 0.05) - Stronger steering response\n",
      "MAX_ALPHA: 1.0 (was 0.4) - Higher maximum steering\n",
      "Other parameters: KI=0.001, KD=0.001, MAX_I=0.2\n",
      "BASE_TEMP: 0.6, STEER_TEMP: 0.3\n",
      "FLUFF_STAR: 0.5, EMB_LAYER: 20\n",
      "\n",
      "--- RASPID TRACE ---\n",
      "step | on | p_red |  err  |   α   |   I   |   D   | temp | token\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853d82c416704b93991442ed79758c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | '?\\n\\n'\n",
      "   1 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | '<think>'\n",
      "   2 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | '\\n'\n",
      "   3 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | 'To'\n",
      "   4 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' find'\n",
      "   5 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' the'\n",
      "   6 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' square'\n",
      "   7 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' root'\n",
      "   8 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' of'\n",
      "   9 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' '\n",
      "  10 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | '8'\n",
      "  11 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | '1'\n",
      "  12 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ','\n",
      "  13 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' I'\n",
      "  14 | 0 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' need'\n",
      "[INFO] Activating steering at gen_len=15\n",
      "  15 | 1 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' to'\n",
      "  16 | 1 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' determine'\n",
      "  17 | 1 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' the'\n",
      "  18 | 1 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' number'\n",
      "  19 | 1 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' that'\n",
      "  20 | 1 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ','\n",
      "  21 | 1 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' when'\n",
      "  22 | 1 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.600 | ' multiplied'\n",
      "[INFO] Scaling down extreme classifier value: 8622.75 -> 31.65\n",
      "  23 | 1 | 1.000 | 0.500 | 0.076 | 0.000 | 0.000 | 0.600 | ' by'\n",
      "  24 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' itself'\n",
      "  25 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ','\n",
      "  26 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' equals'\n",
      "  27 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' '\n",
      "  28 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | '8'\n",
      "  29 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | '1'\n",
      "  30 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | '.\\n\\n'\n",
      "  31 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | 'I'\n",
      "  32 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ''ll'\n",
      "  33 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' start'\n",
      "  34 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' by'\n",
      "  35 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' testing'\n",
      "  36 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' some'\n",
      "  37 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' whole'\n",
      "  38 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' numbers'\n",
      "  39 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | '.'\n",
      "  40 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' \\n\\n'\n",
      "  41 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | 'First'\n",
      "  42 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ','\n",
      "  43 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' I'\n",
      "  44 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ''ll'\n",
      "  45 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' try'\n",
      "  46 | 1 | 0.000 | 0.000 | 0.076 | 0.000 | 0.000 | 0.577 | ' '\n",
      "[INFO] Scaling down extreme classifier value: 8584.34 -> 31.64\n",
      "  47 | 1 | 1.000 | 0.500 | 0.152 | 0.001 | 0.000 | 0.577 | '9'\n",
      "  48 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | '.'\n",
      "  49 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' Multip'\n",
      "  50 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | 'lying'\n",
      "  51 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' '\n",
      "  52 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | '9'\n",
      "  53 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' by'\n",
      "  54 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' itself'\n",
      "  55 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' gives'\n",
      "  56 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' '\n",
      "  57 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | '8'\n",
      "  58 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | '1'\n",
      "  59 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | '.\\n\\n'\n",
      "  60 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | 'Since'\n",
      "  61 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' '\n",
      "  62 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | '9'\n",
      "  63 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' squared'\n",
      "  64 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' equals'\n",
      "  65 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' '\n",
      "  66 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | '8'\n",
      "  67 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | '1'\n",
      "  68 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ','\n",
      "  69 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' the'\n",
      "  70 | 1 | 0.000 | 0.000 | 0.152 | 0.001 | 0.000 | 0.554 | ' square'\n",
      "[INFO] Scaling down extreme classifier value: 8537.99 -> 31.62\n",
      "  71 | 1 | 1.000 | 0.500 | 0.229 | 0.001 | 0.000 | 0.554 | ' root'\n",
      "  72 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' of'\n",
      "  73 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' '\n",
      "  74 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | '8'\n",
      "  75 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | '1'\n",
      "  76 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' is'\n",
      "  77 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' '\n",
      "  78 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | '9'\n",
      "  79 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | '.\\n'\n",
      "  80 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | '</think>'\n",
      "  81 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | '\\n\\n'\n",
      "  82 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | 'To'\n",
      "  83 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' find'\n",
      "  84 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' the'\n",
      "  85 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' square'\n",
      "  86 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' root'\n",
      "  87 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' of'\n",
      "  88 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' '\n",
      "  89 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | '8'\n",
      "  90 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | '1'\n",
      "  91 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ','\n",
      "  92 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' we'\n",
      "  93 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' need'\n",
      "  94 | 1 | 0.000 | 0.000 | 0.229 | 0.001 | 0.000 | 0.531 | ' to'\n",
      "[INFO] Scaling down extreme classifier value: 8616.95 -> 31.64\n",
      "  95 | 1 | 1.000 | 0.500 | 0.307 | 0.002 | 0.000 | 0.531 | ' determine'\n",
      "  96 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' a'\n",
      "  97 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' number'\n",
      "  98 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' that'\n",
      "  99 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ','\n",
      " 100 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' when'\n",
      " 101 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' multiplied'\n",
      " 102 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' by'\n",
      " 103 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' itself'\n",
      " 104 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ','\n",
      " 105 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' equals'\n",
      " 106 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' '\n",
      " 107 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | '8'\n",
      " 108 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | '1'\n",
      " 109 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | '.\\n\\n'\n",
      " 110 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | 'Let'\n",
      " 111 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ''s'\n",
      " 112 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' test'\n",
      " 113 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' some'\n",
      " 114 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' whole'\n",
      " 115 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' numbers'\n",
      " 116 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ':\\n\\n'\n",
      " 117 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | '-'\n",
      " 118 | 1 | 0.000 | 0.000 | 0.307 | 0.002 | 0.000 | 0.508 | ' \\('\n",
      "[INFO] Scaling down extreme classifier value: 8717.90 -> 31.67\n",
      " 119 | 1 | 1.000 | 0.500 | 0.385 | 0.002 | 0.000 | 0.508 | '9'\n",
      " 120 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | ' \\'\n",
      " 121 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | 'times'\n",
      " 122 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | ' '\n",
      " 123 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '9'\n",
      " 124 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | ' ='\n",
      " 125 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | ' '\n",
      " 126 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '8'\n",
      " 127 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '1'\n",
      " 128 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '\\'\n",
      " 129 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | ')\\n\\n'\n",
      " 130 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | 'Since'\n",
      " 131 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | ' \\('\n",
      " 132 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '9'\n",
      " 133 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '^'\n",
      " 134 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '2'\n",
      " 135 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | ' ='\n",
      " 136 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | ' '\n",
      " 137 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '8'\n",
      " 138 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '1'\n",
      " 139 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '\\'\n",
      " 140 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | '),'\n",
      " 141 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | ' the'\n",
      " 142 | 1 | 0.000 | 0.000 | 0.385 | 0.002 | 0.000 | 0.485 | ' square'\n",
      "[INFO] Scaling down extreme classifier value: 8699.62 -> 31.67\n",
      " 143 | 1 | 1.000 | 0.500 | 0.463 | 0.003 | 0.000 | 0.485 | ' root'\n",
      " 144 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | ' of'\n",
      " 145 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | ' '\n",
      " 146 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | '8'\n",
      " 147 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | '1'\n",
      " 148 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | ' is'\n",
      " 149 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | ':\\n\\n'\n",
      " 150 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | '\\'\n",
      " 151 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | '[\\n'\n",
      " 152 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | '\\'\n",
      " 153 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | 'boxed'\n",
      " 154 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | '{'\n",
      " 155 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | '9'\n",
      " 156 | 1 | 0.000 | 0.000 | 0.463 | 0.003 | 0.000 | 0.461 | '}\\n'\n",
      "[INFO] Stop condition met, ending generation\n",
      "--- END TRACE ---\n",
      "\n",
      "=== GENERATION RESULT ===\n",
      "Total tokens generated: 157\n",
      "Final text: what is square root of 81?\n",
      "\n",
      "<think>\n",
      "To find the square root of 81, I need to determine the number that, when multiplied by itself, equals 81.\n",
      "\n",
      "I'll start by testing some whole numbers. \n",
      "\n",
      "First, I'll try 9. Multiplying 9 by itself gives 81.\n",
      "\n",
      "Since 9 squared equals 81, the square root of 81 is 9.\n",
      "</think>\n",
      "\n",
      "To find the square root of 81, we need to determine a number that, when multiplied by itself, equals 81.\n",
      "\n",
      "Let's test some whole numbers:\n",
      "\n",
      "- \\(9 \\times 9 = 81\\)\n",
      "\n",
      "Since \\(9^2 = 81\\), the square root of 81 is:\n",
      "\n",
      "\\[\n",
      "\\boxed{9}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = generate_raspid_fixed_v2('what is square root of 81')\n",
    "b = generate_baseline('what is square root of 81')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "94a24e80-80f0-424c-9802-d95bc7effc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"what is square root of 81?\\n\\n<think>\\nTo find the square root of 81, I need to determine the number that, when multiplied by itself, equals 81.\\n\\nI'll start by testing some whole numbers. \\n\\nFirst, I'll try 9. Multiplying 9 by itself gives 81.\\n\\nSince 9 squared equals 81, the square root of 81 is 9.\\n</think>\\n\\nTo find the square root of 81, we need to determine a number that, when multiplied by itself, equals 81.\\n\\nLet's test some whole numbers:\\n\\n- \\\\(9 \\\\times 9 = 81\\\\)\\n\\nSince \\\\(9^2 = 81\\\\), the square root of 81 is:\\n\\n\\\\[\\n\\\\boxed{9}\\n\",\n",
       " 157)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f51a2636-cc07-4ef3-b175-703b2ef9c62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"what is square root of 81?\\n\\n<think>\\nTo find the square root of 81, I need to determine which number multiplied by itself equals 81.\\n\\nI know that multiplying a number by itself results in its square. For example:\\n\\n- \\\\(9 \\\\times 9 = 81\\\\)\\n\\nTherefore, the square root of 81 is 9.\\n</think>\\n\\n**Solution:**\\n\\nTo find the **square root** of 81, we look for a number that, when multiplied by itself, gives us 81.\\n\\nLet's denote this unknown number as \\\\( x \\\\). Therefore, according to our equation:\\n\\\\[ \\nx^2 = 81 \\n\\\\]\\n\\nWe can solve for \\\\( x \\\\) by taking the square root of both sides:\\n\\\\[\\nx = \\\\sqrt{81}\\n\\\\]\\n\\nSince \\\\( 9 \\\\times 9 = 81 \\\\), it follows that:\\n\\\\[\\n\\\\sqrt{81} = 9\\n\\\\]\\n\\nThus, the square root of 81 is:\\n\\n\\\\[\\n\\\\boxed{9}\\n\\\\]\",\n",
       " 217)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af1c80-6a73-47f5-8cc6-fee8086393e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
